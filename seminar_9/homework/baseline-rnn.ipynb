{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import re\n",
    "import urllib.request\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "class ImdbMovieReviews:\n",
    "    \"\"\"\n",
    "    The movie review dataset is offered by Stanford Universityâ€™s AI department:\n",
    "    http://ai.stanford.edu/~amaas/data/sentiment/. It comes as a compressed  tar  archive where\n",
    "    positive and negative reviews can be found as text files in two according folders. We apply\n",
    "    the same pre-processing to the text as in the last section: Extracting plain words using a\n",
    "    regular expression and converting to lower case.\n",
    "    \"\"\"\n",
    "    DEFAULT_URL = \\\n",
    "        'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "    TOKEN_REGEX = re.compile(r'[A-Za-z]+|[!?.:,()]')\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._cache_dir = './imdb'\n",
    "        self._url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "        \n",
    "        if not os.path.isfile(self._cache_dir):\n",
    "            urllib.request.urlretrieve(self._url, self._cache_dir)\n",
    "        self.filepath = self._cache_dir\n",
    "\n",
    "    def __iter__(self):\n",
    "        with tarfile.open(self.filepath) as archive:\n",
    "            items = archive.getnames()\n",
    "            for filename in archive.getnames():\n",
    "                if filename.startswith('aclImdb/train/pos/'):\n",
    "                    yield self._read(archive, filename), True\n",
    "                elif filename.startswith('aclImdb/train/neg/'):\n",
    "                    yield self._read(archive, filename), False\n",
    "                    \n",
    "    def _read(self, archive, filename):\n",
    "        with archive.extractfile(filename) as file_:\n",
    "            data = file_.read().decode('utf-8')\n",
    "            data = type(self).TOKEN_REGEX.findall(data)\n",
    "            data = [x.lower() for x in data]\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Spacy is my favourite nlp framework, which havu builtin word embeddings trains on wikipesia\n",
    "from spacy.en import English\n",
    "\n",
    "class Embedding:\n",
    "    \n",
    "    def __init__(self, length):\n",
    "#          spaCy makes using word vectors very easy. \n",
    "#             The Lexeme , Token , Span  and Doc  classes all have a .vector property,\n",
    "#             which is a 1-dimensional numpy array of 32-bit floats:\n",
    "        self.parser = English()\n",
    "        self._length = length\n",
    "        self.dimensions = 300\n",
    "        \n",
    "    def __call__(self, sequence):\n",
    "        data = np.zeros((self._length, self.dimensions))\n",
    "        # you can access known words from the parser's vocabulary\n",
    "        embedded = [self.parser.vocab[w].vector for w in sequence]\n",
    "        data[:len(sequence)] = embedded\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lazy import lazy\n",
    "\n",
    "class SequenceClassificationModel:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self._create_placeholders()\n",
    "        self.prediction\n",
    "        self.cost\n",
    "        self.error\n",
    "        self.optimize\n",
    "        self.global_step = 0\n",
    "        self._create_summaries()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _create_placeholders(self):\n",
    "        with tf.name_scope(\"data\"):\n",
    "            self.data = tf.placeholder(tf.float32, [None, self.params.seq_length, self.params.embed_length])\n",
    "            self.target = tf.placeholder(tf.float32, [None, 2])\n",
    "  \n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar('loss', self.cost)\n",
    "            tf.summary.scalar('erroe', self.error)\n",
    "            self.summary = tf.summary.merge_all()\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "    @lazy\n",
    "    def length(self):\n",
    "        with tf.name_scope(\"seq_length\"):\n",
    "            used = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "            length = tf.reduce_sum(used, reduction_indices=1)\n",
    "            length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "    \n",
    "    @lazy\n",
    "    def prediction(self):\n",
    "        with tf.name_scope(\"recurrent_layer\"):\n",
    "            output, _ = tf.nn.dynamic_rnn(\n",
    "                self.params.rnn_cell(self.params.rnn_hidden),\n",
    "                self.data,\n",
    "                dtype=tf.float32,\n",
    "                sequence_length=self.length\n",
    "            )\n",
    "        last = self._last_relevant(output, self.length)\n",
    "\n",
    "        with tf.name_scope(\"softmax_layer\"):\n",
    "            num_classes = int(self.target.get_shape()[1])\n",
    "            weight = tf.Variable(tf.truncated_normal(\n",
    "                [self.params.rnn_hidden, num_classes], stddev=0.01))\n",
    "            bias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "            prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)\n",
    "        return prediction\n",
    "    \n",
    "    @lazy\n",
    "    def cost(self):\n",
    "        cross_entropy = -tf.reduce_sum(self.target * tf.log(self.prediction))\n",
    "        return cross_entropy\n",
    "    \n",
    "    @lazy\n",
    "    def error(self):\n",
    "        self.mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 1), tf.argmax(self.prediction, 1))\n",
    "        return tf.reduce_mean(tf.cast(self.mistakes, tf.float32))\n",
    "    \n",
    "    @lazy\n",
    "    def optimize(self):\n",
    "        with tf.name_scope(\"optimization\"):\n",
    "            gradient = self.params.optimizer.compute_gradients(self.cost)\n",
    "            if self.params.gradient_clipping:\n",
    "                limit = self.params.gradient_clipping\n",
    "                gradient = [\n",
    "                    (tf.clip_by_value(g, -limit, limit), v)\n",
    "                    if g is not None else (None, v)\n",
    "                    for g, v in gradient]\n",
    "            optimize = self.params.optimizer.apply_gradients(gradient)\n",
    "        return optimize\n",
    "    \n",
    "    @staticmethod\n",
    "    def _last_relevant(output, length):\n",
    "        with tf.name_scope(\"last_relevant\"):\n",
    "            # As of now, TensorFlow only supports indexing along the first dimension, using\n",
    "            # tf.gather() . We thus flatten the first two dimensions of the output activations from their\n",
    "            # shape of  sequences x time_steps x word_vectors  and construct an index into this resulting tensor.\n",
    "            batch_size = tf.shape(output)[0]\n",
    "            max_length = int(output.get_shape()[1])\n",
    "            output_size = int(output.get_shape()[2])\n",
    "\n",
    "            # The index takes into account the start indices for each sequence in the flat tensor and adds\n",
    "            # the sequence length to it. Actually, we only add  length - 1  so that we select the last valid\n",
    "            # time step.\n",
    "            index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "            flat = tf.reshape(output, [-1, output_size])\n",
    "            relevant = tf.gather(flat, index)\n",
    "        return relevant\n",
    "    \n",
    "    def train(self, batches, save_prefix, save_every=10):\n",
    "        saver = tf.train.Saver()\n",
    "        if os.path.isdir('./saved/'):\n",
    "            saver.restore(self.sess, tf.train.latest_checkpoint('./saved/'))\n",
    "        else:\n",
    "            os.makedirs('saved')\n",
    "        summary_writer = tf.summary.FileWriter('graphs/run{}'.format(self.global_step), self.sess.graph)\n",
    "        self.global_step += 1\n",
    "        for index, batch in enumerate(batches):\n",
    "            feed = {model.data: batch[0], model.target: batch[1]}\n",
    "            error, _, summary_str = self.sess.run([model.error, model.optimize, model.summary], feed)\n",
    "            print('{}: {:3.1f}%'.format(index + 1, 100 * error))\n",
    "            if index % save_every == 0:\n",
    "                summary_writer.add_summary(summary_str, index)\n",
    "                summary_writer.flush()\n",
    "            if index % save_every == 0:\n",
    "                save_path = os.path.join('checkpoints', save_prefix)\n",
    "                print('saving...', save_path)\n",
    "                saver.save(self.sess, save_path, global_step=index)\n",
    "        saver.save(self.sess, os.path.join('checkpoints', save_prefix + '_final'))\n",
    "\n",
    "    def predict_proba(self, data):\n",
    "        feed = {model.data: data, }\n",
    "        prediction = self.sess.run([model.prediction], feed)        \n",
    "        return prediction\n",
    "    \n",
    "    def predict(self, batches, save_prefix):\n",
    "        saver = tf.train.Saver()\n",
    "        if os.path.isdir('./saved/'):\n",
    "            saver.restore(self.sess, tf.train.latest_checkpoint('./saved/'))\n",
    "        else:\n",
    "            os.makedirs('saved')\n",
    "        summary_writer = tf.summary.FileWriter('graphs/run{}'.format(self.global_step), self.sess.graph)\n",
    "        self.global_step += 1\n",
    "        for index, batch in enumerate(batches):\n",
    "            feed = {model.data: batch[0], model.target: batch[1]}\n",
    "            error, _, summary_str = self.sess.run([model.error, model.optimize, model.summary], feed)\n",
    "            print('{}: {:3.1f}%'.format(index + 1, 100 * error))\n",
    "            if index % save_every == 0:\n",
    "                summary_writer.add_summary(summary_str, index)\n",
    "                summary_writer.flush()\n",
    "            if index % save_every == 0:\n",
    "                save_path = os.path.join('checkpoints', save_prefix)\n",
    "                print('saving...', save_path)\n",
    "                saver.save(self.sess, save_path, global_step=index)\n",
    "        saver.save(self.sess, os.path.join('checkpoints', save_prefix + '_final'))\n",
    "\n",
    "    \n",
    "    def close(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_batched(iterator, length, embedding, batch_size):\n",
    "    iterator = iter(iterator)\n",
    "    while True:\n",
    "        data = np.zeros((batch_size, length, embedding.dimensions))\n",
    "        target = np.zeros((batch_size, 2))\n",
    "        for index in range(batch_size):\n",
    "            text, label = next(iterator)\n",
    "            data[index] = embedding(text)\n",
    "            target[index] = [1, 0] if label else [0, 1]\n",
    "        yield data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = list(ImdbMovieReviews())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = max(len(x[0]) for x in reviews)\n",
    "embedding = Embedding(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attrdict import AttrDict\n",
    "\n",
    "params = AttrDict(\n",
    "    rnn_cell=tf.contrib.rnn.GRUCell,\n",
    "    rnn_hidden=300,\n",
    "    optimizer=tf.train.RMSPropOptimizer(0.002),\n",
    "    batch_size=20,\n",
    "    gradient_clipping=100,\n",
    "    seq_length=length,\n",
    "    embed_length=embedding.dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = preprocess_batched(reviews, length, embedding, params.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = SequenceClassificationModel(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from no_att_checkpoints/simple-rnn-1240\n",
      "INFO:tensorflow:Restoring parameters from no_att_checkpoints/simple-rnn-1240\n",
      "at batch 0 accuracy is 0.9000\n",
      "at batch 1 accuracy is 0.8750\n",
      "at batch 2 accuracy is 0.8667\n",
      "at batch 3 accuracy is 0.8750\n",
      "at batch 4 accuracy is 0.8700\n",
      "at batch 5 accuracy is 0.8500\n",
      "at batch 6 accuracy is 0.8714\n",
      "at batch 7 accuracy is 0.8688\n",
      "at batch 8 accuracy is 0.8778\n",
      "at batch 9 accuracy is 0.8850\n",
      "at batch 10 accuracy is 0.8864\n",
      "at batch 11 accuracy is 0.8875\n",
      "at batch 12 accuracy is 0.8923\n",
      "at batch 13 accuracy is 0.8786\n",
      "at batch 14 accuracy is 0.8700\n",
      "at batch 15 accuracy is 0.8781\n",
      "at batch 16 accuracy is 0.8824\n",
      "at batch 17 accuracy is 0.8833\n",
      "at batch 18 accuracy is 0.8895\n",
      "at batch 19 accuracy is 0.8875\n",
      "at batch 20 accuracy is 0.8881\n",
      "at batch 21 accuracy is 0.8886\n",
      "at batch 22 accuracy is 0.8913\n",
      "at batch 23 accuracy is 0.8896\n",
      "at batch 24 accuracy is 0.8860\n",
      "at batch 25 accuracy is 0.8827\n",
      "at batch 26 accuracy is 0.8833\n",
      "at batch 27 accuracy is 0.8839\n",
      "at batch 28 accuracy is 0.8862\n",
      "at batch 29 accuracy is 0.8833\n",
      "at batch 30 accuracy is 0.8790\n",
      "at batch 31 accuracy is 0.8781\n",
      "at batch 32 accuracy is 0.8788\n",
      "at batch 33 accuracy is 0.8779\n",
      "at batch 34 accuracy is 0.8771\n",
      "at batch 35 accuracy is 0.8778\n",
      "at batch 36 accuracy is 0.8770\n",
      "at batch 37 accuracy is 0.8763\n",
      "at batch 38 accuracy is 0.8756\n",
      "at batch 39 accuracy is 0.8738\n",
      "at batch 40 accuracy is 0.8732\n",
      "at batch 41 accuracy is 0.8714\n",
      "at batch 42 accuracy is 0.8721\n",
      "at batch 43 accuracy is 0.8682\n",
      "at batch 44 accuracy is 0.8700\n",
      "at batch 45 accuracy is 0.8685\n",
      "at batch 46 accuracy is 0.8713\n",
      "at batch 47 accuracy is 0.8698\n",
      "at batch 48 accuracy is 0.8704\n",
      "at batch 49 accuracy is 0.8700\n",
      "at batch 50 accuracy is 0.8706\n",
      "at batch 51 accuracy is 0.8702\n",
      "at batch 52 accuracy is 0.8698\n",
      "at batch 53 accuracy is 0.8722\n",
      "at batch 54 accuracy is 0.8727\n",
      "at batch 55 accuracy is 0.8714\n",
      "at batch 56 accuracy is 0.8737\n",
      "at batch 57 accuracy is 0.8750\n",
      "at batch 58 accuracy is 0.8763\n",
      "at batch 59 accuracy is 0.8775\n",
      "at batch 60 accuracy is 0.8754\n",
      "at batch 61 accuracy is 0.8750\n",
      "at batch 62 accuracy is 0.8738\n",
      "at batch 63 accuracy is 0.8711\n",
      "at batch 64 accuracy is 0.8685\n",
      "at batch 65 accuracy is 0.8674\n",
      "at batch 66 accuracy is 0.8672\n",
      "at batch 67 accuracy is 0.8654\n",
      "at batch 68 accuracy is 0.8645\n",
      "at batch 69 accuracy is 0.8657\n",
      "at batch 70 accuracy is 0.8662\n",
      "at batch 71 accuracy is 0.8667\n",
      "at batch 72 accuracy is 0.8678\n",
      "at batch 73 accuracy is 0.8682\n",
      "at batch 74 accuracy is 0.8680\n",
      "at batch 75 accuracy is 0.8684\n",
      "at batch 76 accuracy is 0.8688\n",
      "at batch 77 accuracy is 0.8692\n",
      "at batch 78 accuracy is 0.8696\n",
      "at batch 79 accuracy is 0.8694\n",
      "at batch 80 accuracy is 0.8698\n",
      "at batch 81 accuracy is 0.8695\n",
      "at batch 82 accuracy is 0.8705\n",
      "at batch 83 accuracy is 0.8708\n",
      "at batch 84 accuracy is 0.8712\n",
      "at batch 85 accuracy is 0.8727\n",
      "at batch 86 accuracy is 0.8730\n",
      "at batch 87 accuracy is 0.8733\n",
      "at batch 88 accuracy is 0.8747\n",
      "at batch 89 accuracy is 0.8750\n",
      "at batch 90 accuracy is 0.8753\n",
      "at batch 91 accuracy is 0.8761\n",
      "at batch 92 accuracy is 0.8763\n",
      "at batch 93 accuracy is 0.8761\n",
      "at batch 94 accuracy is 0.8768\n",
      "at batch 95 accuracy is 0.8776\n",
      "at batch 96 accuracy is 0.8768\n",
      "at batch 97 accuracy is 0.8760\n",
      "at batch 98 accuracy is 0.8758\n",
      "at batch 99 accuracy is 0.8760\n",
      "at batch 100 accuracy is 0.8752\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "checkpoint_dir = 'no_att_checkpoints'\n",
    "checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "print(\"Reading model parameters from %s\" % checkpoint.model_checkpoint_path)\n",
    "saver.restore(model.sess, checkpoint.model_checkpoint_path)\n",
    "\n",
    "trues = 0\n",
    "alls = 0\n",
    "\n",
    "for i, batch in enumerate(batches):\n",
    "    y_true = batch[1]\n",
    "    preds = model.sess.run(model.prediction, {model.data: batch[0]})\n",
    "    trues += (y_true == np.round(preds)).sum() / 2\n",
    "    alls += len(y_true)\n",
    "    print('at batch {} accuracy is {:.4f}'.format(i, trues / alls))\n",
    "    if i == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
