{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import my_colors\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import re\n",
    "import urllib.request\n",
    "import os\n",
    "import random\n",
    "\n",
    "class ImdbMovieReviews:\n",
    "    DEFAULT_URL = \\\n",
    "        'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "    TOKEN_REGEX = re.compile(r'[A-Za-z]+|[!?.:,()]')\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._cache_dir = './imdb'\n",
    "        self._url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "        \n",
    "        if not os.path.isfile(self._cache_dir):\n",
    "            urllib.request.urlretrieve(self._url, self._cache_dir)\n",
    "        self.filepath = self._cache_dir\n",
    "\n",
    "    def __iter__(self):\n",
    "        with tarfile.open(self.filepath) as archive:\n",
    "            items = archive.getnames()\n",
    "            for filename in archive.getnames():\n",
    "                if filename.startswith('aclImdb/train/pos/'):\n",
    "                    yield self._read(archive, filename), True\n",
    "                elif filename.startswith('aclImdb/train/neg/'):\n",
    "                    yield self._read(archive, filename), False\n",
    "                    \n",
    "    def _read(self, archive, filename):\n",
    "        with archive.extractfile(filename) as file_:\n",
    "            data = file_.read().decode('utf-8')\n",
    "            data = type(self).TOKEN_REGEX.findall(data)\n",
    "            data = [x.lower() for x in data]\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Spacy is my favourite nlp framework, which havu builtin word embeddings trains on wikipesia\n",
    "from spacy.en import English\n",
    "\n",
    "class Embedding:\n",
    "    \n",
    "    def __init__(self):\n",
    "#          spaCy makes using word vectors very easy. \n",
    "#             The Lexeme , Token , Span  and Doc  classes all have a .vector property,\n",
    "#             which is a 1-dimensional numpy array of 32-bit floats:\n",
    "        self.parser = English()\n",
    "#         self._length = length\n",
    "        self.dimensions = 300\n",
    "        \n",
    "    def __call__(self, sequence, length):\n",
    "        # DO I really need them to be equal length?\n",
    "        # Let's assume I'm not\n",
    "        data = np.zeros((length, self.dimensions))\n",
    "        # you can access known words from the parser's vocabulary\n",
    "        embedded = [self.parser.vocab[w].vector for w in sequence]\n",
    "        data[:len(sequence)] = embedded\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def preprocess_batched_split(iterator, embedding, batch_size):\n",
    "    iterator = iter(iterator)\n",
    "    while True:\n",
    "        batch = []\n",
    "        labelss = []\n",
    "        sentence_sizes_batch = []\n",
    "        for index in range(batch_size):\n",
    "            text, label = next(iterator)\n",
    "            sents = [list(y) for x, y in itertools.groupby(text, lambda z: z == '.') if not x]\n",
    "            sentence_sizes = [len(s) for s in sents]\n",
    "            text_embed = [embedding(sent) for sent in sents]\n",
    "            \n",
    "            batch.append(text_embed)\n",
    "            labelss.append(label)\n",
    "            sentence_sizes_batch.append(sentence_sizes)\n",
    "            \n",
    "        labels_batch = np.array(labelss, dtype=np.int32)\n",
    "        sent_per_doc = np.array([len(x) for x in sentence_sizes_batch])\n",
    "        words_per_sent_per_doc = np.array(sentence_sizes_batch)\n",
    "        yield np.array(batch), labels_batch, words_per_sent_per_doc, sent_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def preprocess_batched_split2(iterator, embedding, batch_size):\n",
    "    iterator = iter(iterator)\n",
    "    while True:\n",
    "        batch, labels_b = zip(*itertools.islice(iterator, batch_size))\n",
    "        \n",
    "        sents_b = [[list(y) for x, y in itertools.groupby(doc, lambda z: z == '.') if not x] for doc in batch]\n",
    "\n",
    "        sentence_sizes_b = [[len(sent) for sent in doc] for doc in sents_b]\n",
    "        sentence_size = max(map(max, sentence_sizes_b))\n",
    "        \n",
    "        document_sizes = np.array([len(doc) for doc in sentence_sizes_b], dtype=np.int32)\n",
    "        document_size = document_sizes.max()\n",
    "\n",
    "        sentence_sizes_np = np.zeros(shape=[batch_size, document_size], dtype=np.int32)\n",
    "        for bi, ds, ss in zip(range(sentence_sizes_np.shape[0]), document_sizes, sentence_sizes_b):\n",
    "            sentence_sizes_np[bi][:ds] = ss\n",
    "        \n",
    "        text_embed_b = np.zeros((batch_size, document_size, sentence_size, 300))\n",
    "        for i, ds, doc_sents in zip(range(text_embed_b.shape[0]), document_sizes, sents_b):\n",
    "            doc_sents_embed = np.array([embedding(sent, sentence_size) for sent in doc_sents])\n",
    "            text_embed_b[i][:ds] = doc_sents_embed\n",
    "        \n",
    "        yield text_embed_b, np.array(labels_b, dtype=np.int32), np.array(document_sizes), sentence_sizes_np, sents_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = list(ImdbMovieReviews())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import HanSequenceLabellingModel\n",
    "import model_components\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "#%aimport HanSequenceLabellingModel, model_components\n",
    "%aimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches_split = preprocess_batched_split2(reviews, Embedding(), batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from HanSequenceLabellingModel import HanSequenceLabellingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def HAN_model_1(session, restore_only=False):\n",
    "    \"\"\"Hierarhical Attention Network\"\"\"\n",
    "    import tensorflow as tf\n",
    "    try:\n",
    "        from tensorflow.contrib.rnn import GRUCell, MultiRNNCell, DropoutWrapper\n",
    "    except ImportError:\n",
    "        MultiRNNCell = tf.nn.rnn_cell.MultiRNNCell\n",
    "        GRUCell = tf.nn.rnn_cell.GRUCell\n",
    "    from bn_lstm import BNLSTMCell\n",
    "    from HanSequenceLabellingModel import HanSequenceLabellingModel\n",
    "\n",
    "    is_training = tf.placeholder(dtype=tf.bool, name='is_training')\n",
    "\n",
    "    cell = BNLSTMCell(80, is_training) # h-h batchnorm LSTMCell\n",
    "    cell = MultiRNNCell([cell]*5)\n",
    "\n",
    "    model = HanSequenceLabellingModel(\n",
    "            embedding_size=300,\n",
    "            classes=2,\n",
    "            word_cell=cell,\n",
    "            sentence_cell=cell,\n",
    "            word_output_size=300,\n",
    "            sentence_output_size=300,\n",
    "            learning_rate=0.001,\n",
    "            max_grad_norm=5.0,\n",
    "            dropout_keep_proba=0.5,\n",
    "            is_training=is_training,\n",
    "    )\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if checkpoint:\n",
    "        print(\"Reading model parameters from %s\" % checkpoint.model_checkpoint_path)\n",
    "        saver.restore(session, checkpoint.model_checkpoint_path)\n",
    "    elif restore_only:\n",
    "        raise FileNotFoundError(\"Cannot restore model\")\n",
    "    else:\n",
    "        print(\"Created model with fresh parameters\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "    return model, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from checkpoints/checkpoint-2400\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/checkpoint-2400\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "5 is not between 0 and 4\n\t [[Node: train/gradients/sentence_level/sentence/attention/Tensordot/transpose_grad/InvertPermutation = InvertPermutation[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](sentence_level/sentence/attention/Tensordot/concat_1)]]\n\t [[Node: sentence_level/sentence/attention/Tensordot/Shape/_1777 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_34752_sentence_level/sentence/attention/Tensordot/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'train/gradients/sentence_level/sentence/attention/Tensordot/transpose_grad/InvertPermutation', defined at:\n  File \"/opt/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/opt/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/opt/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-cafc7deabc02>\", line 6, in <module>\n    model, saver = HAN_model_1(s)\n  File \"<ipython-input-18-3fd959f4f9c4>\", line 27, in HAN_model_1\n    is_training=is_training,\n  File \"/home/katenos/au_dl_course/seminar_9/homework/HanSequenceLabellingModel.py\", line 39, in __init__\n    self.train_op\n  File \"/opt/anaconda3/lib/python3.6/site-packages/lazy/lazy.py\", line 28, in __get__\n    value = self.__func(inst)\n  File \"/home/katenos/au_dl_course/seminar_9/homework/HanSequenceLabellingModel.py\", line 141, in train_op\n    tf.gradients(self.loss, tvars),\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 542, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 348, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 542, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py\", line 494, in _TransposeGrad\n    return [array_ops.transpose(grad, array_ops.invert_permutation(p)), None]\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1454, in invert_permutation\n    result = _op_def_lib.apply_op(\"InvertPermutation\", x=x, name=name)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'sentence_level/sentence/attention/Tensordot/transpose', defined at:\n  File \"/opt/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 20 identical lines from previous traceback]\n  File \"<ipython-input-18-3fd959f4f9c4>\", line 27, in HAN_model_1\n    is_training=is_training,\n  File \"/home/katenos/au_dl_course/seminar_9/homework/HanSequenceLabellingModel.py\", line 37, in __init__\n    self.sentence_level_output\n  File \"/opt/anaconda3/lib/python3.6/site-packages/lazy/lazy.py\", line 28, in __get__\n    value = self.__func(inst)\n  File \"/home/katenos/au_dl_course/seminar_9/homework/HanSequenceLabellingModel.py\", line 113, in sentence_level_output\n    sentence_encoder_output, self.sentence_output_size, scope=scope)\n  File \"/home/katenos/au_dl_course/seminar_9/homework/model_components.py\", line 82, in task_specific_attention\n    scope=scope)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1661, in fully_connected\n    outputs = layer.apply(inputs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 503, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 450, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/core.py\", line 137, in call\n    [0]])\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2453, in tensordot\n    a_reshape, a_free_dims, a_free_dims_static = _tensordot_reshape(a, a_axes)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2420, in _tensordot_reshape\n    reshaped_a = array_ops.reshape(array_ops.transpose(a, perm), new_shape)\n\nInvalidArgumentError (see above for traceback): 5 is not between 0 and 4\n\t [[Node: train/gradients/sentence_level/sentence/attention/Tensordot/transpose_grad/InvertPermutation = InvertPermutation[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](sentence_level/sentence/attention/Tensordot/concat_1)]]\n\t [[Node: sentence_level/sentence/attention/Tensordot/Shape/_1777 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_34752_sentence_level/sentence/attention/Tensordot/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 5 is not between 0 and 4\n\t [[Node: train/gradients/sentence_level/sentence/attention/Tensordot/transpose_grad/InvertPermutation = InvertPermutation[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](sentence_level/sentence/attention/Tensordot/concat_1)]]\n\t [[Node: sentence_level/sentence/attention/Tensordot/Shape/_1777 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_34752_sentence_level/sentence/attention/Tensordot/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cafc7deabc02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         ], feed_dict=fd)\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mtd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 5 is not between 0 and 4\n\t [[Node: train/gradients/sentence_level/sentence/attention/Tensordot/transpose_grad/InvertPermutation = InvertPermutation[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](sentence_level/sentence/attention/Tensordot/concat_1)]]\n\t [[Node: sentence_level/sentence/attention/Tensordot/Shape/_1777 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_34752_sentence_level/sentence/attention/Tensordot/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'train/gradients/sentence_level/sentence/attention/Tensordot/transpose_grad/InvertPermutation', defined at:\n  File \"/opt/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/opt/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/opt/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-cafc7deabc02>\", line 6, in <module>\n    model, saver = HAN_model_1(s)\n  File \"<ipython-input-18-3fd959f4f9c4>\", line 27, in HAN_model_1\n    is_training=is_training,\n  File \"/home/katenos/au_dl_course/seminar_9/homework/HanSequenceLabellingModel.py\", line 39, in __init__\n    self.train_op\n  File \"/opt/anaconda3/lib/python3.6/site-packages/lazy/lazy.py\", line 28, in __get__\n    value = self.__func(inst)\n  File \"/home/katenos/au_dl_course/seminar_9/homework/HanSequenceLabellingModel.py\", line 141, in train_op\n    tf.gradients(self.loss, tvars),\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 542, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 348, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 542, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py\", line 494, in _TransposeGrad\n    return [array_ops.transpose(grad, array_ops.invert_permutation(p)), None]\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1454, in invert_permutation\n    result = _op_def_lib.apply_op(\"InvertPermutation\", x=x, name=name)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'sentence_level/sentence/attention/Tensordot/transpose', defined at:\n  File \"/opt/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 20 identical lines from previous traceback]\n  File \"<ipython-input-18-3fd959f4f9c4>\", line 27, in HAN_model_1\n    is_training=is_training,\n  File \"/home/katenos/au_dl_course/seminar_9/homework/HanSequenceLabellingModel.py\", line 37, in __init__\n    self.sentence_level_output\n  File \"/opt/anaconda3/lib/python3.6/site-packages/lazy/lazy.py\", line 28, in __get__\n    value = self.__func(inst)\n  File \"/home/katenos/au_dl_course/seminar_9/homework/HanSequenceLabellingModel.py\", line 113, in sentence_level_output\n    sentence_encoder_output, self.sentence_output_size, scope=scope)\n  File \"/home/katenos/au_dl_course/seminar_9/homework/model_components.py\", line 82, in task_specific_attention\n    scope=scope)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1661, in fully_connected\n    outputs = layer.apply(inputs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 503, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 450, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/core.py\", line 137, in call\n    [0]])\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2453, in tensordot\n    a_reshape, a_free_dims, a_free_dims_static = _tensordot_reshape(a, a_axes)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 2420, in _tensordot_reshape\n    reshaped_a = array_ops.reshape(array_ops.transpose(a, perm), new_shape)\n\nInvalidArgumentError (see above for traceback): 5 is not between 0 and 4\n\t [[Node: train/gradients/sentence_level/sentence/attention/Tensordot/transpose_grad/InvertPermutation = InvertPermutation[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](sentence_level/sentence/attention/Tensordot/concat_1)]]\n\t [[Node: sentence_level/sentence/attention/Tensordot/Shape/_1777 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_34752_sentence_level/sentence/attention/Tensordot/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n",
    "\n",
    "with tf.Session(config=config) as s:\n",
    "    model, saver = HAN_model_1(s)\n",
    "    tflog_dir = 'tf_logs'\n",
    "    summary_writer = tf.summary.FileWriter(tflog_dir, graph=tf.get_default_graph())\n",
    "\n",
    "    for i, (data, labels_batch, sent_per_doc, words_per_sent_per_doc,) in enumerate(batches_split):\n",
    "\n",
    "        fd = {\n",
    "            model.is_training: True,\n",
    "            model.inputs_embedded: data,\n",
    "            model.word_lengths: words_per_sent_per_doc,\n",
    "            model.sentence_lengths: sent_per_doc,\n",
    "            model.labels: labels_batch,\n",
    "            model.sample_weights: np.ones(shape=(10))\n",
    "        }\n",
    "\n",
    "        t0 = time.clock()\n",
    "        step, summaries, loss, accuracy, _ = s.run([\n",
    "                model.global_step,\n",
    "                model.summary,\n",
    "                model.loss,\n",
    "                model.accuracy,\n",
    "                model.train_op,\n",
    "        ], feed_dict=fd)\n",
    "        td = time.clock() - t0\n",
    "\n",
    "        summary_writer.add_summary(summaries, global_step=step)\n",
    "\n",
    "        checkpoint_frequency = 100\n",
    "        eval_frequency = 1\n",
    "        \n",
    "        if step % 1 == 0:\n",
    "            print('step %s, loss=%s, accuracy=%s, t=%s, inputs=%s' % (step, loss, accuracy, round(td, 2), fd[model.inputs_embedded].shape))\n",
    "        if step != 0 and step % checkpoint_frequency == 0:\n",
    "            print('checkpoint & graph meta')\n",
    "            checkpoint_path = 'checkpoints/checkpoint'\n",
    "            saver.save(s, checkpoint_path, global_step=step)\n",
    "            print('checkpoint done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from checkpoints/checkpoint-2400\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/checkpoint-2400\n",
      "\u001b[48;5;201m\t \u001b[0m\u001b[48;5;198m\t \u001b[0m\u001b[48;5;196m\t\n",
      "\u001b[0mmin\tmed\tmax\n",
      "\u001b[48;5;200m\t \u001b[0m  \u001b[0m\u001b[48;5;200mthis \u001b[0m\u001b[48;5;200mmovie \u001b[0m\u001b[48;5;200mshould \u001b[0m\u001b[48;5;201mbe \u001b[0m\u001b[48;5;201mshown \u001b[0m\u001b[48;5;201mto \u001b[0m\u001b[48;5;201mfilm \u001b[0m\u001b[48;5;201mschool \u001b[0m\u001b[48;5;201mstudents \u001b[0m\u001b[48;5;201mas \u001b[0m\u001b[48;5;201man \u001b[0m\u001b[48;5;201mexample \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201mwhat \u001b[0m\u001b[48;5;201mnot \u001b[0m\u001b[48;5;201mto \u001b[0m\u001b[48;5;201mdo \u001b[0m\n",
      "\u001b[48;5;196m\t \u001b[0m  \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201moriginal \u001b[0m\u001b[48;5;201mkicked \u001b[0m\u001b[48;5;201msome \u001b[0m\u001b[48;5;201mmajor \u001b[0m\u001b[48;5;201mtire \u001b[0m\u001b[48;5;201msquealing \u001b[0m\u001b[48;5;201mbutt \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;200mthis \u001b[0m\u001b[48;5;200mhorrible \u001b[0m\u001b[48;5;201mdisaster \u001b[0m\u001b[48;5;201mbreaks \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mcardinal \u001b[0m\u001b[48;5;201mrule \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201mbruckheimer \u001b[0m\u001b[48;5;201mfilms \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mwhich \u001b[0m\u001b[48;5;201mis \u001b[0m\u001b[48;5;201m: \u001b[0m\u001b[48;5;201mwe \u001b[0m\u001b[48;5;201mall \u001b[0m\u001b[48;5;201mknow \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201msuck \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mbut \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mhave \u001b[0m\u001b[48;5;201mgreat \u001b[0m\u001b[48;5;201maction \u001b[0m\n",
      "\u001b[48;5;198m\t \u001b[0m  \u001b[0m\u001b[48;5;200mthis \u001b[0m\u001b[48;5;200mfilm \u001b[0m\u001b[48;5;199mhas \u001b[0m\u001b[48;5;199mno \u001b[0m\u001b[48;5;200maction \u001b[0m\n",
      "\u001b[48;5;197m\t \u001b[0m  \u001b[0m\u001b[48;5;200mthis \u001b[0m\u001b[48;5;200mfilm \u001b[0m\u001b[48;5;198mis \u001b[0m\u001b[48;5;196mboring \u001b[0m\n",
      "\u001b[48;5;197m\t \u001b[0m  \u001b[0m\u001b[48;5;201mwhere \u001b[0m\u001b[48;5;201mare \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mcars \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;201mwhere \u001b[0m\u001b[48;5;201mare \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mchases \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;201mwhere \u001b[0m\u001b[48;5;201ms \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mtension \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;201mwhere \u001b[0m\u001b[48;5;201ms \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201msuspense \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;201mwhere \u001b[0m\u001b[48;5;201ms \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mrush \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;201mwhere \u001b[0m\u001b[48;5;201m! \u001b[0m\u001b[48;5;201m! \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;201mthis \u001b[0m\u001b[48;5;201misn \u001b[0m\u001b[48;5;201mt \u001b[0m\u001b[48;5;201mreally \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mmovie \u001b[0m\u001b[48;5;201mat \u001b[0m\u001b[48;5;201mall \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mit \u001b[0m\u001b[48;5;201ms \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;200mbad \u001b[0m\u001b[48;5;201mcommercial \u001b[0m\n",
      "\u001b[48;5;198m\t \u001b[0m  \u001b[0m\u001b[48;5;201mcars \u001b[0m\u001b[48;5;201min \u001b[0m\u001b[48;5;201mhours \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;201mthat \u001b[0m\u001b[48;5;201mis \u001b[0m\u001b[48;5;200mwrong \u001b[0m\n",
      "\u001b[48;5;198m\t \u001b[0m  \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mhave \u001b[0m\u001b[48;5;201mdays \u001b[0m\u001b[48;5;201mto \u001b[0m\u001b[48;5;201msteal \u001b[0m\u001b[48;5;201mthem \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mad \u001b[0m\u001b[48;5;201mis \u001b[0m\u001b[48;5;200mwrong \u001b[0m\n",
      "\u001b[48;5;197m\t \u001b[0m  \u001b[0m\u001b[48;5;201mhow \u001b[0m\u001b[48;5;199mbad \u001b[0m\u001b[48;5;200mis \u001b[0m\u001b[48;5;200mthat \u001b[0m\u001b[48;5;200m? \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mleads \u001b[0m\u001b[48;5;201macting \u001b[0m\u001b[48;5;201mis \u001b[0m\u001b[48;5;201mstiff \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mwooden \u001b[0m\u001b[48;5;201mand \u001b[0m\u001b[48;5;201mforced \u001b[0m\n",
      "\u001b[48;5;198m\t \u001b[0m  \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;200mvillain \u001b[0m\u001b[48;5;200m, \u001b[0m\u001b[48;5;200mthe \u001b[0m\u001b[48;5;201mcop \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mothers \u001b[0m\n",
      "\u001b[48;5;199m\t \u001b[0m  \u001b[0m\u001b[48;5;201mwho \u001b[0m\u001b[48;5;200mcares \u001b[0m\n",
      "\u001b[48;5;198m\t \u001b[0m  \u001b[0m\u001b[48;5;200mthey \u001b[0m\u001b[48;5;199mutter \u001b[0m\u001b[48;5;200mtheir \u001b[0m\u001b[48;5;199mpointless \u001b[0m\u001b[48;5;200mlines \u001b[0m\u001b[48;5;200m, \u001b[0m\u001b[48;5;200mthey \u001b[0m\u001b[48;5;201mserve \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201millogical \u001b[0m\u001b[48;5;201mplot \u001b[0m\n",
      "\u001b[48;5;199m\t \u001b[0m  \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mslog \u001b[0m\u001b[48;5;201mthrough \u001b[0m\u001b[48;5;201mit \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mbest \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mcan \u001b[0m\u001b[48;5;201mas \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mmusic \u001b[0m\u001b[48;5;201mvideo \u001b[0m\u001b[48;5;201mdirector \u001b[0m\u001b[48;5;201msays \u001b[0m\u001b[48;5;201mdon \u001b[0m\u001b[48;5;201mt \u001b[0m\u001b[48;5;201mworry \u001b[0m\u001b[48;5;201mwe \u001b[0m\u001b[48;5;201mll \u001b[0m\u001b[48;5;201mmake \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mlot \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201mfast \u001b[0m\u001b[48;5;201mcuts \u001b[0m\u001b[48;5;201mand \u001b[0m\u001b[48;5;201mno \u001b[0m\u001b[48;5;201mone \u001b[0m\u001b[48;5;201mwill \u001b[0m\u001b[48;5;201mnotice \u001b[0m\u001b[48;5;201mhow \u001b[0m\u001b[48;5;201mbad \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mfilm \u001b[0m\u001b[48;5;201mis \u001b[0m\u001b[48;5;201mor \u001b[0m\u001b[48;5;201mwe \u001b[0m\u001b[48;5;201mll \u001b[0m\u001b[48;5;201mfix \u001b[0m\u001b[48;5;201mit \u001b[0m\u001b[48;5;201mwith \u001b[0m\u001b[48;5;201mlots \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201mloud \u001b[0m\u001b[48;5;201mmusic \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mscript \u001b[0m\u001b[48;5;201misn \u001b[0m\u001b[48;5;201mt \u001b[0m\u001b[48;5;201mreally \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mscript \u001b[0m\u001b[48;5;201mat \u001b[0m\u001b[48;5;201mall \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mit \u001b[0m\u001b[48;5;201ms \u001b[0m\u001b[48;5;201mmore \u001b[0m\u001b[48;5;201mlike \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mlist \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201mcliches \u001b[0m\u001b[48;5;201mwith \u001b[0m\u001b[48;5;201man \u001b[0m\u001b[48;5;201mending \u001b[0m\u001b[48;5;201mthat \u001b[0m\u001b[48;5;201mis \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mtotal \u001b[0m\u001b[48;5;201mripoff \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201m: \u001b[0m\u001b[48;5;201mwarning \u001b[0m\u001b[48;5;201mpossible \u001b[0m\u001b[48;5;201mspoiler \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mfugitive \u001b[0m\n",
      "\u001b[48;5;200m\t \u001b[0m  \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mbiggest \u001b[0m\u001b[48;5;201mcrime \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201mall \u001b[0m\u001b[48;5;201mis \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201munderuse \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201mvinnie \u001b[0m\u001b[48;5;201mjones \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mman \u001b[0m\n",
      "\u001b[48;5;200m\t \u001b[0m  \u001b[0m\u001b[48;5;200mthis \u001b[0m\u001b[48;5;200mis \u001b[0m\u001b[48;5;200mthe \u001b[0m\u001b[48;5;200mbaddest \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mcoolest \u001b[0m\u001b[48;5;201mmofo \u001b[0m\u001b[48;5;201msince \u001b[0m\u001b[48;5;201mjules \u001b[0m\u001b[48;5;201min \u001b[0m\u001b[48;5;201mpulp \u001b[0m\u001b[48;5;201mfiction \u001b[0m\n",
      "\u001b[48;5;200m\t \u001b[0m  \u001b[0m\u001b[48;5;201mand \u001b[0m\u001b[48;5;201mwhat \u001b[0m\u001b[48;5;201mdo \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mdo \u001b[0m\u001b[48;5;201m! \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mmake \u001b[0m\u001b[48;5;201mhim \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mmute \u001b[0m\u001b[48;5;201mwho \u001b[0m\u001b[48;5;201ms \u001b[0m\u001b[48;5;201mhardly \u001b[0m\u001b[48;5;201min \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mfilm \u001b[0m\u001b[48;5;201m! \u001b[0m\u001b[48;5;201mmake \u001b[0m\u001b[48;5;201mvinnie \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mmain \u001b[0m\u001b[48;5;201mvillain \u001b[0m\u001b[48;5;201mand \u001b[0m\u001b[48;5;201mhe \u001b[0m\u001b[48;5;201mcould \u001b[0m\u001b[48;5;201mhave \u001b[0m\u001b[48;5;201msaved \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mfilm \u001b[0m\n",
      "\u001b[48;5;200m\t \u001b[0m  \u001b[0m\u001b[48;5;201mhow \u001b[0m\u001b[48;5;201mcould \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mhave \u001b[0m\u001b[48;5;201mbeen \u001b[0m\u001b[48;5;201mso \u001b[0m\u001b[48;5;201mdumb \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;201mhow \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;201mhow \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;201mwhy \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;200mthe \u001b[0m\u001b[48;5;200moriginal \u001b[0m\u001b[48;5;200mfilm \u001b[0m\u001b[48;5;200mis \u001b[0m\u001b[48;5;201mvery \u001b[0m\u001b[48;5;201mentertaining \u001b[0m\u001b[48;5;201mwith \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mcool \u001b[0m\u001b[48;5;201mtrick \u001b[0m\u001b[48;5;201mat \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mend \u001b[0m\u001b[48;5;201mthat \u001b[0m\u001b[48;5;201mgets \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mdriver \u001b[0m\u001b[48;5;201maway \u001b[0m\n",
      "\u001b[48;5;200m\t \u001b[0m  \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201moriginal \u001b[0m\u001b[48;5;201mhas \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;200mgreat \u001b[0m\u001b[48;5;201mminute \u001b[0m\u001b[48;5;201mchase \u001b[0m\u001b[48;5;201mthat \u001b[0m\u001b[48;5;201mdelivers \u001b[0m\u001b[48;5;201m! \u001b[0m\u001b[48;5;201mgo \u001b[0m\u001b[48;5;201mfind \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201moriginal \u001b[0m\n",
      "\u001b[48;5;200m\t \u001b[0m  \u001b[0m\u001b[48;5;201mor \u001b[0m\u001b[48;5;201mif \u001b[0m\u001b[48;5;201myou \u001b[0m\u001b[48;5;201mre \u001b[0m\u001b[48;5;201mcraving \u001b[0m\u001b[48;5;201msome \u001b[0m\u001b[48;5;201mreal \u001b[0m\u001b[48;5;201mcar \u001b[0m\u001b[48;5;201mchase \u001b[0m\u001b[48;5;201maction \u001b[0m\u001b[48;5;201mgo \u001b[0m\u001b[48;5;201mrent \u001b[0m\u001b[48;5;201mronin \u001b[0m\n",
      "\u001b[48;5;201m\t \u001b[0m  \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mchases \u001b[0m\u001b[48;5;201min \u001b[0m\u001b[48;5;201mronin \u001b[0m\u001b[48;5;201mraised \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mbar \u001b[0m\u001b[48;5;201mby \u001b[0m\u001b[48;5;201mwhich \u001b[0m\u001b[48;5;201mall \u001b[0m\u001b[48;5;201mother \u001b[0m\u001b[48;5;201mcar \u001b[0m\u001b[48;5;201mchases \u001b[0m\u001b[48;5;201mwill \u001b[0m\u001b[48;5;201mnow \u001b[0m\u001b[48;5;201mbe \u001b[0m\u001b[48;5;201mjudged \u001b[0m\n",
      "\u001b[48;5;200m\t \u001b[0m  \u001b[0m\u001b[48;5;201mbruckheimer \u001b[0m\u001b[48;5;201mand \u001b[0m\u001b[48;5;201mcage \u001b[0m\u001b[48;5;201mhad \u001b[0m\u001b[48;5;201mall \u001b[0m\u001b[48;5;201mthat \u001b[0m\u001b[48;5;201mmoney \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mall \u001b[0m\u001b[48;5;201mthose \u001b[0m\u001b[48;5;201mresources \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mall \u001b[0m\u001b[48;5;201mthat \u001b[0m\u001b[48;5;201mexperience \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mand \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mcan \u001b[0m\u001b[48;5;201mt \u001b[0m\u001b[48;5;201meven \u001b[0m\u001b[48;5;201mcome \u001b[0m\u001b[48;5;201mclose \u001b[0m\u001b[48;5;201mto \u001b[0m\u001b[48;5;201mmatching \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mfilm \u001b[0m\u001b[48;5;201mmade \u001b[0m\u001b[48;5;201myears \u001b[0m\u001b[48;5;201mago \u001b[0m\u001b[48;5;201mfor \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;201mhow \u001b[0m\u001b[48;5;201mcan \u001b[0m\u001b[48;5;201mthat \u001b[0m\u001b[48;5;201mbe \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;201myou \u001b[0m\u001b[48;5;201mfeel \u001b[0m\u001b[48;5;201mlike \u001b[0m\u001b[48;5;201myou \u001b[0m\u001b[48;5;201mgot \u001b[0m\u001b[48;5;201mripped \u001b[0m\u001b[48;5;201moff \u001b[0m\u001b[48;5;201mafter \u001b[0m\u001b[48;5;201mseeing \u001b[0m\u001b[48;5;201mthis \u001b[0m\u001b[48;5;201mmovie \u001b[0m\n",
      "\u001b[48;5;200m\t \u001b[0m  \u001b[0m\u001b[48;5;201mwhere \u001b[0m\u001b[48;5;201mi \u001b[0m\u001b[48;5;201mwas \u001b[0m\u001b[48;5;201monce \u001b[0m\u001b[48;5;201mexcited \u001b[0m\u001b[48;5;201mto \u001b[0m\u001b[48;5;201msee \u001b[0m\u001b[48;5;201mcoyote \u001b[0m\u001b[48;5;200mugly \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mremember \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mtitans \u001b[0m\u001b[48;5;201mand \u001b[0m\u001b[48;5;201mpearl \u001b[0m\u001b[48;5;201mharbor \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mnow \u001b[0m\u001b[48;5;201mi \u001b[0m\u001b[48;5;201msay \u001b[0m\u001b[48;5;201m: \u001b[0m\u001b[48;5;201mgod \u001b[0m\u001b[48;5;201mhelp \u001b[0m\u001b[48;5;201mus \u001b[0m\u001b[48;5;201mall \u001b[0m\n",
      "\u001b[48;5;200m\t \u001b[0m  \u001b[0m\u001b[48;5;201mbr \u001b[0m\u001b[48;5;201mbr \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n",
    "\n",
    "from my_colors import print_color, rgb\n",
    "\n",
    "\n",
    "        \n",
    "with tf.Session(config=config) as s:\n",
    "    model, saver = HAN_model_1(s)\n",
    "    tflog_dir = 'tf_logs'\n",
    "    summary_writer = tf.summary.FileWriter(tflog_dir, graph=tf.get_default_graph())\n",
    "    \n",
    "    data, labels_batch, sent_per_doc,\\\n",
    "    words_per_sent_per_doc, sents_batch = next(batches_split)\n",
    "\n",
    "    fd = {\n",
    "        model.is_training: True,\n",
    "        model.inputs_embedded: data,\n",
    "        model.word_lengths: words_per_sent_per_doc,\n",
    "        model.sentence_lengths: sent_per_doc,\n",
    "        model.labels: labels_batch,\n",
    "        model.sample_weights: np.ones(shape=(10))\n",
    "    }\n",
    "\n",
    "    word_weights, sent_weights = s.run([model.word_weights, \n",
    "                                              model.sentence_weights], \n",
    "                                             feed_dict=fd)\n",
    "    \n",
    "    sent_weights = sent_weights[0]\n",
    "    sents = sents_batch[0]\n",
    "    \n",
    "    max_sent_weight = 0\n",
    "    max_word_weight = 0\n",
    "    \n",
    "    for sent_index in range(len(sents)):\n",
    "        max_sent_weight = max(max_sent_weight, sent_weights[sent_index])\n",
    "        \n",
    "        for word_index in range(len(sents[sent_index])):\n",
    "            max_word_weight = max(max_word_weight, word_weights[sent_index][word_index])\n",
    "    \n",
    "    def draw_colored_text(sentences, sent_weights, word_weights):\n",
    "        print_color(\"\\t\", bg=rgb(5, 0, 5.0), end=' ') \n",
    "        print_color(\"\\t\", bg=rgb(5, 0, 2.5), end=' ')\n",
    "        print_color(\"\\t\", bg=rgb(5, 0, 0.0), end='\\n')\n",
    "        print(\"min\\tmed\\tmax\")\n",
    "        for sent_index in range(len(sentences)):\n",
    "            intensity =  5 - int(sent_weights[sent_index] / max_sent_weight*5)\n",
    "            print_color(\"\\t\", bg=rgb(5, 0, intensity), end=' ') \n",
    "            print_color(\" \", end=' ') \n",
    "            for word_index in range(len(sentences[sent_index])):\n",
    "                intensity = 5 - int(word_weights[sent_index][word_index] / max_word_weight*5)\n",
    "                print_color(sentences[sent_index][word_index], bg=rgb(5, 0, intensity), end=' ') \n",
    "            print()\n",
    "    draw_colored_text(sents,  sent_weights, word_weights)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goods_in_pos = []\n",
    "goods_in_neg = []\n",
    "bads_in_pos = []\n",
    "bads_in_neg = []\n",
    "\n",
    "def update(status, word, weight):\n",
    "    def update_pos(word):\n",
    "        if word == 'good':\n",
    "            goods_in_pos.append(weight[0])\n",
    "        else:\n",
    "            bads_in_pos.append(weight[0])\n",
    "    def update_neg(word):\n",
    "        if word == 'good':\n",
    "            goods_in_neg.append(weight[0])\n",
    "        else:\n",
    "            bads_in_neg.append(weight[0])\n",
    "\n",
    "    update_pos(word) if status else update_neg(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from checkpoints/checkpoint-2400\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/checkpoint-2400\n",
      "300\r"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n",
    "\n",
    "with tf.Session(config=config) as s:\n",
    "    model, saver = HAN_model_1(s)\n",
    "    tflog_dir = 'tf_logs'\n",
    "    summary_writer = tf.summary.FileWriter(tflog_dir, graph=tf.get_default_graph())\n",
    "    \n",
    "    for i, (data, labels_batch, sent_per_doc,\\\n",
    "            words_per_sent_per_doc, sents_batch) in enumerate(batches_split):\n",
    "\n",
    "        fd = {\n",
    "            model.is_training: True,\n",
    "            model.inputs_embedded: data,\n",
    "            model.word_lengths: words_per_sent_per_doc,\n",
    "            model.sentence_lengths: sent_per_doc,\n",
    "            model.labels: labels_batch,\n",
    "            model.sample_weights: np.ones(shape=(10))\n",
    "        }\n",
    "\n",
    "        word_weights, sent_weights = s.run([model.word_weights, \n",
    "                                                  model.sentence_weights], \n",
    "                                                 feed_dict=fd)\n",
    "\n",
    "        for review_index in range(len(sents_batch)):\n",
    "            status = labels_batch[review_index]\n",
    "            for sent_index in range(len(sents_batch[review_index])):\n",
    "                for word_index in range(len(sents_batch[review_index][sent_index])):\n",
    "                    word = sents_batch[review_index][sent_index][word_index]\n",
    "                    if word == 'good' or word == 'bad':\n",
    "                        weight = word_weights[review_index * sent_weights.shape[1]][word_index]\n",
    "                        update(status, word, weight)\n",
    "        \n",
    "        print(i, end='\\r')\n",
    "        \n",
    "        if i == 300:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIBToIiKBUKCLiATi/wMHaH0kjyyfvwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f334b3eea90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkUAo0EVEAqFAFxEJxP8DHLf91BfDC3EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f334b3eee48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "def draw_plot(pos, neg, word):\n",
    "    max_att = max(max(pos), max(neg))\n",
    "    bins = np.linspace(0, max_att, 50)   \n",
    "    \n",
    "    plt.hist(pos, bins=bins, color='lightblue', \n",
    "             alpha=0.9, label='{} in positives'.format(word), \n",
    "             normed=True)\n",
    "    plt.hist(neg, bins=bins, color='salmon', \n",
    "             alpha=0.3, label='{} in negatives'.format(word),\n",
    "             normed=True)\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "draw_plot(goods_in_pos, goods_in_neg, 'good')\n",
    "draw_plot(bads_in_pos, bads_in_neg, 'bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
