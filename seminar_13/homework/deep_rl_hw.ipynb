{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tflearn\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend.tensorflow_backend as KTF\n",
    "import os \n",
    "import tensorflow as tf\n",
    "\n",
    "def get_session(gpu_fraction=0.3):\n",
    "\n",
    "    num_threads = os.environ.get('OMP_NUM_THREADS')\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "\n",
    "    if num_threads:\n",
    "        return tf.Session(config=tf.ConfigProto(\n",
    "            gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))\n",
    "    else:\n",
    "        return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "\n",
    "KTF.set_session(get_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: fill empty spaces in the following agent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepQAgent:\n",
    "    def __init__(self, state_size, action_size, render=False):\n",
    "        # Tip: if you are training this on AWS the best way is to turn off rendering\n",
    "        # and load it later with the serialized model\n",
    "        self.render = render\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.005\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / 50000\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # replay memory\n",
    "        self.memory = deque(maxlen=20000)\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Use tflearn to get simple NN for deep q-learning\n",
    "        # Spoler alert: a couple of fully connected hidden layers should be enough\n",
    "        # Output layer should have the same dimensionality as the action space\n",
    "\n",
    "        inputs = Input(shape=(self.state_size,))\n",
    "        net = Dense(64, activation='relu')(inputs)\n",
    "        net = Dense(64, activation='relu')(net)\n",
    "        preds = Dense(self.action_size, activation='linear')(net)\n",
    "        model = Model(inputs=inputs, outputs=preds)\n",
    "        \n",
    "        opt = adam(lr=self.learning_rate)\n",
    "        model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Update your target model to the model you are currently learning at regular time intervals\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"The choice of action uses the epsilon-greedy policy for the current network.\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def replay_memory(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save <s, a, r, s'> to replay_memory\"\"\"\n",
    "        if action == 2:\n",
    "            action = 1\n",
    "        self.memory.append([state, action, reward, next_state, done])\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "\n",
    "    def train_replay(self):\n",
    "        \"\"\"Random samplin            if action == 0:\n",
    "                fake_action = 0\n",
    "            elif action == 1:\n",
    "                fake_action = 2g of batch_size samples from replay memory\"\"\"\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.action_size))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            state, action, reward, next_state, done = mini_batch[i]\n",
    "            target = self.model.predict(state)[0]\n",
    "\n",
    "            # As in queuing, it gets the maximum Q Value at s'. However, it is imported from the target model.\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                target[action] = reward + self.discount_factor * \\\n",
    "                                        np.amax(self.target_model.predict(next_state)[0])\n",
    "            update_input[i] = state\n",
    "            update_target[i] = target\n",
    "\n",
    "        # You can create a minibatch of the correct target answer and the current value of your own,\n",
    "        self.model.fit(update_input, update_target, batch_size=batch_size, epochs=1, verbose=0)\n",
    "\n",
    "    def load_model(self, name):\n",
    "        self.model.load_model(name)\n",
    "\n",
    "    def save_model(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state_size = env.observation_space.shape[0] # should be equal 2\n",
    "ACTION_SIZE = 2\n",
    "agent = DeepQAgent(state_size, ACTION_SIZE)\n",
    "# agent.load_model(\"./save_model/<your_saved_model_name>\")\n",
    "scores, episodes = [], []\n",
    "N_EPISODES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: -200.0   memory length: 200   epsilon: 0.9960200000000077\n",
      "episode: 1   score: -200.0   memory length: 400   epsilon: 0.9920400000000154\n",
      "episode: 2   score: -200.0   memory length: 600   epsilon: 0.988060000000023\n",
      "episode: 3   score: -200.0   memory length: 800   epsilon: 0.9840800000000307\n",
      "episode: 4   score: -200.0   memory length: 1000   epsilon: 0.9801000000000384\n",
      "episode: 5   score: -200.0   memory length: 1200   epsilon: 0.9761200000000461\n",
      "episode: 6   score: -200.0   memory length: 1400   epsilon: 0.9721400000000537\n",
      "episode: 7   score: -200.0   memory length: 1600   epsilon: 0.9681600000000614\n",
      "episode: 8   score: -200.0   memory length: 1800   epsilon: 0.9641800000000691\n",
      "episode: 9   score: -200.0   memory length: 2000   epsilon: 0.9602000000000768\n",
      "episode: 10   score: -200.0   memory length: 2200   epsilon: 0.9562200000000844\n",
      "episode: 11   score: -200.0   memory length: 2400   epsilon: 0.9522400000000921\n",
      "episode: 12   score: -200.0   memory length: 2600   epsilon: 0.9482600000000998\n",
      "episode: 13   score: -200.0   memory length: 2800   epsilon: 0.9442800000001075\n",
      "episode: 14   score: -200.0   memory length: 3000   epsilon: 0.9403000000001152\n",
      "episode: 15   score: -200.0   memory length: 3200   epsilon: 0.9363200000001228\n",
      "episode: 16   score: -200.0   memory length: 3400   epsilon: 0.9323400000001305\n",
      "episode: 17   score: -200.0   memory length: 3600   epsilon: 0.9283600000001382\n",
      "episode: 18   score: -200.0   memory length: 3800   epsilon: 0.9243800000001459\n",
      "episode: 19   score: -200.0   memory length: 4000   epsilon: 0.9204000000001535\n",
      "episode: 20   score: -200.0   memory length: 4200   epsilon: 0.9164200000001612\n",
      "episode: 21   score: -200.0   memory length: 4400   epsilon: 0.9124400000001689\n",
      "episode: 22   score: -200.0   memory length: 4600   epsilon: 0.9084600000001766\n",
      "episode: 23   score: -200.0   memory length: 4800   epsilon: 0.9044800000001842\n",
      "episode: 24   score: -200.0   memory length: 5000   epsilon: 0.9005000000001919\n",
      "episode: 25   score: -200.0   memory length: 5200   epsilon: 0.8965200000001996\n",
      "episode: 26   score: -200.0   memory length: 5400   epsilon: 0.8925400000002073\n",
      "episode: 27   score: -200.0   memory length: 5600   epsilon: 0.888560000000215\n",
      "episode: 28   score: -200.0   memory length: 5800   epsilon: 0.8845800000002226\n",
      "episode: 29   score: -200.0   memory length: 6000   epsilon: 0.8806000000002303\n",
      "episode: 30   score: -200.0   memory length: 6200   epsilon: 0.876620000000238\n",
      "episode: 31   score: -200.0   memory length: 6400   epsilon: 0.8726400000002457\n",
      "episode: 32   score: -200.0   memory length: 6600   epsilon: 0.8686600000002533\n",
      "episode: 33   score: -200.0   memory length: 6800   epsilon: 0.864680000000261\n",
      "episode: 34   score: -200.0   memory length: 7000   epsilon: 0.8607000000002687\n",
      "episode: 35   score: -200.0   memory length: 7200   epsilon: 0.8567200000002764\n",
      "episode: 36   score: -200.0   memory length: 7400   epsilon: 0.852740000000284\n",
      "episode: 37   score: -200.0   memory length: 7600   epsilon: 0.8487600000002917\n",
      "episode: 38   score: -200.0   memory length: 7800   epsilon: 0.8447800000002994\n",
      "episode: 39   score: -200.0   memory length: 8000   epsilon: 0.8408000000003071\n",
      "episode: 40   score: -200.0   memory length: 8200   epsilon: 0.8368200000003148\n",
      "episode: 41   score: -200.0   memory length: 8400   epsilon: 0.8328400000003224\n",
      "episode: 42   score: -200.0   memory length: 8600   epsilon: 0.8288600000003301\n",
      "episode: 43   score: -200.0   memory length: 8800   epsilon: 0.8248800000003378\n",
      "episode: 44   score: -200.0   memory length: 9000   epsilon: 0.8209000000003455\n",
      "episode: 45   score: -200.0   memory length: 9200   epsilon: 0.8169200000003531\n",
      "episode: 46   score: -200.0   memory length: 9400   epsilon: 0.8129400000003608\n",
      "episode: 47   score: -200.0   memory length: 9600   epsilon: 0.8089600000003685\n",
      "episode: 48   score: -200.0   memory length: 9800   epsilon: 0.8049800000003762\n",
      "episode: 49   score: -200.0   memory length: 10000   epsilon: 0.8010000000003838\n",
      "episode: 50   score: -200.0   memory length: 10200   epsilon: 0.7970200000003915\n",
      "episode: 51   score: -200.0   memory length: 10400   epsilon: 0.7930400000003992\n",
      "episode: 52   score: -200.0   memory length: 10600   epsilon: 0.7890600000004069\n",
      "episode: 53   score: -200.0   memory length: 10800   epsilon: 0.7850800000004146\n",
      "episode: 54   score: -200.0   memory length: 11000   epsilon: 0.7811000000004222\n",
      "episode: 55   score: -200.0   memory length: 11200   epsilon: 0.7771200000004299\n",
      "episode: 56   score: -200.0   memory length: 11400   epsilon: 0.7731400000004376\n",
      "episode: 57   score: -200.0   memory length: 11600   epsilon: 0.7691600000004453\n",
      "episode: 58   score: -200.0   memory length: 11800   epsilon: 0.7651800000004529\n",
      "episode: 59   score: -200.0   memory length: 12000   epsilon: 0.7612000000004606\n",
      "episode: 60   score: -200.0   memory length: 12200   epsilon: 0.7572200000004683\n",
      "episode: 61   score: -200.0   memory length: 12400   epsilon: 0.753240000000476\n",
      "episode: 62   score: -200.0   memory length: 12600   epsilon: 0.7492600000004837\n",
      "episode: 63   score: -200.0   memory length: 12800   epsilon: 0.7452800000004913\n",
      "episode: 64   score: -165.0   memory length: 12965   epsilon: 0.7419965000004977\n",
      "episode: 65   score: -200.0   memory length: 13165   epsilon: 0.7380165000005053\n",
      "episode: 66   score: -200.0   memory length: 13365   epsilon: 0.734036500000513\n",
      "episode: 67   score: -200.0   memory length: 13565   epsilon: 0.7300565000005207\n",
      "episode: 68   score: -200.0   memory length: 13765   epsilon: 0.7260765000005284\n",
      "episode: 69   score: -200.0   memory length: 13965   epsilon: 0.722096500000536\n",
      "episode: 70   score: -200.0   memory length: 14165   epsilon: 0.7181165000005437\n",
      "episode: 71   score: -200.0   memory length: 14365   epsilon: 0.7141365000005514\n",
      "episode: 72   score: -200.0   memory length: 14565   epsilon: 0.7101565000005591\n",
      "episode: 73   score: -200.0   memory length: 14765   epsilon: 0.7061765000005668\n",
      "episode: 74   score: -171.0   memory length: 14936   epsilon: 0.7027736000005733\n",
      "episode: 75   score: -200.0   memory length: 15136   epsilon: 0.698793600000581\n",
      "episode: 76   score: -200.0   memory length: 15336   epsilon: 0.6948136000005887\n",
      "episode: 77   score: -200.0   memory length: 15536   epsilon: 0.6908336000005963\n",
      "episode: 78   score: -200.0   memory length: 15736   epsilon: 0.686853600000604\n",
      "episode: 79   score: -200.0   memory length: 15936   epsilon: 0.6828736000006117\n",
      "episode: 80   score: -200.0   memory length: 16136   epsilon: 0.6788936000006194\n",
      "episode: 81   score: -200.0   memory length: 16336   epsilon: 0.6749136000006271\n",
      "episode: 82   score: -200.0   memory length: 16536   epsilon: 0.6709336000006347\n",
      "episode: 83   score: -200.0   memory length: 16736   epsilon: 0.6669536000006424\n",
      "episode: 84   score: -200.0   memory length: 16936   epsilon: 0.6629736000006501\n",
      "episode: 85   score: -200.0   memory length: 17136   epsilon: 0.6589936000006578\n",
      "episode: 86   score: -200.0   memory length: 17336   epsilon: 0.6550136000006654\n",
      "episode: 87   score: -200.0   memory length: 17536   epsilon: 0.6510336000006731\n",
      "episode: 88   score: -200.0   memory length: 17736   epsilon: 0.6470536000006808\n",
      "episode: 89   score: -200.0   memory length: 17936   epsilon: 0.6430736000006885\n",
      "episode: 90   score: -168.0   memory length: 18104   epsilon: 0.6397304000006949\n",
      "episode: 91   score: -200.0   memory length: 18304   epsilon: 0.6357504000007026\n",
      "episode: 92   score: -200.0   memory length: 18504   epsilon: 0.6317704000007103\n",
      "episode: 93   score: -200.0   memory length: 18704   epsilon: 0.627790400000718\n",
      "episode: 94   score: -200.0   memory length: 18904   epsilon: 0.6238104000007256\n",
      "episode: 95   score: -184.0   memory length: 19088   epsilon: 0.6201488000007327\n",
      "episode: 96   score: -200.0   memory length: 19288   epsilon: 0.6161688000007404\n",
      "episode: 97   score: -200.0   memory length: 19488   epsilon: 0.612188800000748\n",
      "episode: 98   score: -200.0   memory length: 19688   epsilon: 0.6082088000007557\n",
      "episode: 99   score: -200.0   memory length: 19888   epsilon: 0.6042288000007634\n",
      "episode: 100   score: -200.0   memory length: 20000   epsilon: 0.6002488000007711\n",
      "episode: 101   score: -200.0   memory length: 20000   epsilon: 0.5962688000007788\n",
      "episode: 102   score: -200.0   memory length: 20000   epsilon: 0.5922888000007864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 103   score: -153.0   memory length: 20000   epsilon: 0.5892441000007923\n",
      "episode: 104   score: -200.0   memory length: 20000   epsilon: 0.5852641000008\n",
      "episode: 105   score: -157.0   memory length: 20000   epsilon: 0.582139800000806\n",
      "episode: 106   score: -200.0   memory length: 20000   epsilon: 0.5781598000008137\n",
      "episode: 107   score: -200.0   memory length: 20000   epsilon: 0.5741798000008214\n",
      "episode: 108   score: -200.0   memory length: 20000   epsilon: 0.570199800000829\n",
      "episode: 109   score: -200.0   memory length: 20000   epsilon: 0.5662198000008367\n",
      "episode: 110   score: -200.0   memory length: 20000   epsilon: 0.5622398000008444\n",
      "episode: 111   score: -187.0   memory length: 20000   epsilon: 0.5585185000008516\n",
      "episode: 112   score: -200.0   memory length: 20000   epsilon: 0.5545385000008592\n",
      "episode: 113   score: -200.0   memory length: 20000   epsilon: 0.5505585000008669\n",
      "episode: 114   score: -200.0   memory length: 20000   epsilon: 0.5465785000008746\n",
      "episode: 115   score: -200.0   memory length: 20000   epsilon: 0.5425985000008823\n",
      "episode: 116   score: -200.0   memory length: 20000   epsilon: 0.53861850000089\n",
      "episode: 117   score: -129.0   memory length: 20000   epsilon: 0.5360514000008949\n",
      "episode: 118   score: -200.0   memory length: 20000   epsilon: 0.5320714000009026\n",
      "episode: 119   score: -200.0   memory length: 20000   epsilon: 0.5280914000009103\n",
      "episode: 120   score: -200.0   memory length: 20000   epsilon: 0.5241114000009179\n",
      "episode: 121   score: -200.0   memory length: 20000   epsilon: 0.5201314000009256\n",
      "episode: 122   score: -200.0   memory length: 20000   epsilon: 0.5161514000009333\n",
      "episode: 123   score: -200.0   memory length: 20000   epsilon: 0.512171400000941\n",
      "episode: 124   score: -200.0   memory length: 20000   epsilon: 0.5081914000009486\n",
      "episode: 125   score: -168.0   memory length: 20000   epsilon: 0.5048482000009551\n",
      "episode: 126   score: -155.0   memory length: 20000   epsilon: 0.501763700000961\n",
      "episode: 127   score: -153.0   memory length: 20000   epsilon: 0.4987190000009633\n",
      "episode: 128   score: -200.0   memory length: 20000   epsilon: 0.4947390000009599\n",
      "episode: 129   score: -200.0   memory length: 20000   epsilon: 0.49075900000095646\n",
      "episode: 130   score: -192.0   memory length: 20000   epsilon: 0.48693820000095317\n",
      "episode: 131   score: -177.0   memory length: 20000   epsilon: 0.48341590000095014\n",
      "episode: 132   score: -200.0   memory length: 20000   epsilon: 0.4794359000009467\n",
      "episode: 133   score: -176.0   memory length: 20000   epsilon: 0.4759335000009437\n",
      "episode: 134   score: -142.0   memory length: 20000   epsilon: 0.47310770000094127\n",
      "episode: 135   score: -197.0   memory length: 20000   epsilon: 0.4691874000009379\n",
      "episode: 136   score: -152.0   memory length: 20000   epsilon: 0.4661626000009353\n",
      "episode: 137   score: -149.0   memory length: 20000   epsilon: 0.46319750000093274\n",
      "episode: 138   score: -151.0   memory length: 20000   epsilon: 0.46019260000093015\n",
      "episode: 139   score: -161.0   memory length: 20000   epsilon: 0.4569887000009274\n",
      "episode: 140   score: -151.0   memory length: 20000   epsilon: 0.4539838000009248\n",
      "episode: 141   score: -162.0   memory length: 20000   epsilon: 0.45076000000092203\n",
      "episode: 142   score: -139.0   memory length: 20000   epsilon: 0.44799390000091965\n",
      "episode: 143   score: -171.0   memory length: 20000   epsilon: 0.4445910000009167\n",
      "episode: 144   score: -156.0   memory length: 20000   epsilon: 0.44148660000091405\n",
      "episode: 145   score: -137.0   memory length: 20000   epsilon: 0.4387603000009117\n",
      "episode: 146   score: -160.0   memory length: 20000   epsilon: 0.43557630000090897\n",
      "episode: 147   score: -200.0   memory length: 20000   epsilon: 0.43159630000090554\n",
      "episode: 148   score: -145.0   memory length: 20000   epsilon: 0.42871080000090306\n",
      "episode: 149   score: -200.0   memory length: 20000   epsilon: 0.42473080000089963\n",
      "episode: 150   score: -200.0   memory length: 20000   epsilon: 0.4207508000008962\n",
      "episode: 151   score: -200.0   memory length: 20000   epsilon: 0.4167708000008928\n",
      "episode: 152   score: -200.0   memory length: 20000   epsilon: 0.41279080000088936\n",
      "episode: 153   score: -200.0   memory length: 20000   epsilon: 0.40881080000088593\n",
      "episode: 154   score: -166.0   memory length: 20000   epsilon: 0.4055074000008831\n",
      "episode: 155   score: -125.0   memory length: 20000   epsilon: 0.40301990000088095\n",
      "episode: 156   score: -156.0   memory length: 20000   epsilon: 0.3999155000008783\n",
      "episode: 157   score: -163.0   memory length: 20000   epsilon: 0.3966718000008755\n",
      "episode: 158   score: -200.0   memory length: 20000   epsilon: 0.39269180000087206\n",
      "episode: 159   score: -200.0   memory length: 20000   epsilon: 0.38871180000086863\n",
      "episode: 160   score: -183.0   memory length: 20000   epsilon: 0.3850701000008655\n",
      "episode: 161   score: -174.0   memory length: 20000   epsilon: 0.3816075000008625\n",
      "episode: 162   score: -200.0   memory length: 20000   epsilon: 0.3776275000008591\n",
      "episode: 163   score: -200.0   memory length: 20000   epsilon: 0.37364750000085567\n",
      "episode: 164   score: -200.0   memory length: 20000   epsilon: 0.36966750000085224\n",
      "episode: 165   score: -200.0   memory length: 20000   epsilon: 0.3656875000008488\n",
      "episode: 166   score: -200.0   memory length: 20000   epsilon: 0.3617075000008454\n",
      "episode: 167   score: -155.0   memory length: 20000   epsilon: 0.35862300000084274\n",
      "episode: 168   score: -177.0   memory length: 20000   epsilon: 0.3551007000008397\n",
      "episode: 169   score: -166.0   memory length: 20000   epsilon: 0.35179730000083687\n",
      "episode: 170   score: -161.0   memory length: 20000   epsilon: 0.3485934000008341\n",
      "episode: 171   score: -200.0   memory length: 20000   epsilon: 0.3446134000008307\n",
      "episode: 172   score: -174.0   memory length: 20000   epsilon: 0.3411508000008277\n",
      "episode: 173   score: -200.0   memory length: 20000   epsilon: 0.3371708000008243\n",
      "episode: 174   score: -160.0   memory length: 20000   epsilon: 0.33398680000082154\n",
      "episode: 175   score: -147.0   memory length: 20000   epsilon: 0.331061500000819\n",
      "episode: 176   score: -185.0   memory length: 20000   epsilon: 0.32738000000081585\n",
      "episode: 177   score: -200.0   memory length: 20000   epsilon: 0.3234000000008124\n",
      "episode: 178   score: -160.0   memory length: 20000   epsilon: 0.3202160000008097\n",
      "episode: 179   score: -165.0   memory length: 20000   epsilon: 0.31693250000080686\n",
      "episode: 180   score: -200.0   memory length: 20000   epsilon: 0.31295250000080344\n",
      "episode: 181   score: -200.0   memory length: 20000   epsilon: 0.3089725000008\n",
      "episode: 182   score: -136.0   memory length: 20000   epsilon: 0.3062661000007977\n",
      "episode: 183   score: -176.0   memory length: 20000   epsilon: 0.30276370000079467\n",
      "episode: 184   score: -154.0   memory length: 20000   epsilon: 0.29969910000079203\n",
      "episode: 185   score: -148.0   memory length: 20000   epsilon: 0.2967539000007895\n",
      "episode: 186   score: -151.0   memory length: 20000   epsilon: 0.2937490000007869\n",
      "episode: 187   score: -100.0   memory length: 20000   epsilon: 0.2917590000007852\n",
      "episode: 188   score: -200.0   memory length: 20000   epsilon: 0.28777900000078177\n",
      "episode: 189   score: -127.0   memory length: 20000   epsilon: 0.2852517000007796\n",
      "episode: 190   score: -200.0   memory length: 20000   epsilon: 0.28127170000077617\n",
      "episode: 191   score: -148.0   memory length: 20000   epsilon: 0.27832650000077364\n",
      "episode: 192   score: -154.0   memory length: 20000   epsilon: 0.275261900000771\n",
      "episode: 193   score: -150.0   memory length: 20000   epsilon: 0.27227690000076843\n",
      "episode: 194   score: -142.0   memory length: 20000   epsilon: 0.269451100000766\n",
      "episode: 195   score: -167.0   memory length: 20000   epsilon: 0.26612780000076314\n",
      "episode: 196   score: -122.0   memory length: 20000   epsilon: 0.26370000000076105\n",
      "episode: 197   score: -159.0   memory length: 20000   epsilon: 0.2605359000007583\n",
      "episode: 198   score: -146.0   memory length: 20000   epsilon: 0.2576305000007558\n",
      "episode: 199   score: -200.0   memory length: 20000   epsilon: 0.2536505000007524\n",
      "episode: 200   score: -174.0   memory length: 20000   epsilon: 0.2501879000007494\n",
      "episode: 201   score: -140.0   memory length: 20000   epsilon: 0.24740190000075066\n",
      "episode: 202   score: -168.0   memory length: 20000   epsilon: 0.24405870000075244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 203   score: -110.0   memory length: 20000   epsilon: 0.2418697000007536\n",
      "episode: 204   score: -127.0   memory length: 20000   epsilon: 0.23934240000075496\n",
      "episode: 205   score: -157.0   memory length: 20000   epsilon: 0.23621810000075663\n",
      "episode: 206   score: -200.0   memory length: 20000   epsilon: 0.23223810000075876\n",
      "episode: 207   score: -191.0   memory length: 20000   epsilon: 0.2284372000007608\n",
      "episode: 208   score: -156.0   memory length: 20000   epsilon: 0.22533280000076245\n",
      "episode: 209   score: -123.0   memory length: 20000   epsilon: 0.22288510000076375\n",
      "episode: 210   score: -134.0   memory length: 20000   epsilon: 0.22021850000076518\n",
      "episode: 211   score: -186.0   memory length: 20000   epsilon: 0.21651710000076715\n",
      "episode: 212   score: -152.0   memory length: 20000   epsilon: 0.21349230000076877\n",
      "episode: 213   score: -157.0   memory length: 20000   epsilon: 0.21036800000077044\n",
      "episode: 214   score: -146.0   memory length: 20000   epsilon: 0.207462600000772\n",
      "episode: 215   score: -181.0   memory length: 20000   epsilon: 0.20386070000077391\n",
      "episode: 216   score: -132.0   memory length: 20000   epsilon: 0.20123390000077532\n",
      "episode: 217   score: -152.0   memory length: 20000   epsilon: 0.19820910000077693\n",
      "episode: 218   score: -152.0   memory length: 20000   epsilon: 0.19518430000077855\n",
      "episode: 219   score: -147.0   memory length: 20000   epsilon: 0.1922590000007801\n",
      "episode: 220   score: -200.0   memory length: 20000   epsilon: 0.18827900000078224\n",
      "episode: 221   score: -119.0   memory length: 20000   epsilon: 0.1859109000007835\n",
      "episode: 222   score: -117.0   memory length: 20000   epsilon: 0.18358260000078475\n",
      "episode: 223   score: -124.0   memory length: 20000   epsilon: 0.18111500000078606\n",
      "episode: 224   score: -117.0   memory length: 20000   epsilon: 0.1787867000007873\n",
      "episode: 225   score: -134.0   memory length: 20000   epsilon: 0.17612010000078873\n",
      "episode: 226   score: -150.0   memory length: 20000   epsilon: 0.17313510000079033\n",
      "episode: 227   score: -126.0   memory length: 20000   epsilon: 0.17062770000079167\n",
      "episode: 228   score: -142.0   memory length: 20000   epsilon: 0.16780190000079317\n",
      "episode: 229   score: -150.0   memory length: 20000   epsilon: 0.16481690000079477\n",
      "episode: 230   score: -117.0   memory length: 20000   epsilon: 0.162488600000796\n",
      "episode: 231   score: -109.0   memory length: 20000   epsilon: 0.16031950000079717\n",
      "episode: 232   score: -161.0   memory length: 20000   epsilon: 0.15711560000079888\n",
      "episode: 233   score: -145.0   memory length: 20000   epsilon: 0.15423010000080042\n",
      "episode: 234   score: -149.0   memory length: 20000   epsilon: 0.151265000000802\n",
      "episode: 235   score: -147.0   memory length: 20000   epsilon: 0.14833970000080357\n",
      "episode: 236   score: -123.0   memory length: 20000   epsilon: 0.14589200000080488\n",
      "episode: 237   score: -165.0   memory length: 20000   epsilon: 0.14260850000080663\n",
      "episode: 238   score: -170.0   memory length: 20000   epsilon: 0.13922550000080844\n",
      "episode: 239   score: -170.0   memory length: 20000   epsilon: 0.13584250000081025\n",
      "episode: 240   score: -154.0   memory length: 20000   epsilon: 0.13277790000081188\n",
      "episode: 241   score: -157.0   memory length: 20000   epsilon: 0.12965360000081355\n",
      "episode: 242   score: -149.0   memory length: 20000   epsilon: 0.12668850000081514\n",
      "episode: 243   score: -149.0   memory length: 20000   epsilon: 0.12372340000081582\n",
      "episode: 244   score: -153.0   memory length: 20000   epsilon: 0.12067870000081532\n",
      "episode: 245   score: -135.0   memory length: 20000   epsilon: 0.11799220000081488\n",
      "episode: 246   score: -168.0   memory length: 20000   epsilon: 0.11464900000081434\n",
      "episode: 247   score: -158.0   memory length: 20000   epsilon: 0.11150480000081382\n",
      "episode: 248   score: -148.0   memory length: 20000   epsilon: 0.10855960000081334\n",
      "episode: 249   score: -165.0   memory length: 20000   epsilon: 0.1052761000008128\n",
      "episode: 250   score: -93.0   memory length: 20000   epsilon: 0.1034254000008125\n",
      "episode: 251   score: -152.0   memory length: 20000   epsilon: 0.10040060000081201\n",
      "episode: 252   score: -173.0   memory length: 20000   epsilon: 0.09695790000081145\n",
      "episode: 253   score: -161.0   memory length: 20000   epsilon: 0.09375400000081092\n",
      "episode: 254   score: -85.0   memory length: 20000   epsilon: 0.09206250000081065\n",
      "episode: 255   score: -176.0   memory length: 20000   epsilon: 0.08856010000081008\n",
      "episode: 256   score: -172.0   memory length: 20000   epsilon: 0.08513730000080952\n",
      "episode: 257   score: -160.0   memory length: 20000   epsilon: 0.081953300000809\n",
      "episode: 258   score: -161.0   memory length: 20000   epsilon: 0.07874940000080848\n",
      "episode: 259   score: -149.0   memory length: 20000   epsilon: 0.07578430000080799\n",
      "episode: 260   score: -153.0   memory length: 20000   epsilon: 0.0727396000008075\n",
      "episode: 261   score: -154.0   memory length: 20000   epsilon: 0.069675000000807\n",
      "episode: 262   score: -149.0   memory length: 20000   epsilon: 0.06670990000080651\n",
      "episode: 263   score: -149.0   memory length: 20000   epsilon: 0.06374480000080603\n",
      "episode: 264   score: -146.0   memory length: 20000   epsilon: 0.06083940000080555\n",
      "episode: 265   score: -88.0   memory length: 20000   epsilon: 0.059088200000805266\n",
      "episode: 266   score: -142.0   memory length: 20000   epsilon: 0.056262400000804805\n",
      "episode: 267   score: -149.0   memory length: 20000   epsilon: 0.05329730000080432\n",
      "episode: 268   score: -150.0   memory length: 20000   epsilon: 0.05031230000080383\n",
      "episode: 269   score: -86.0   memory length: 20000   epsilon: 0.048600900000803554\n",
      "episode: 270   score: -150.0   memory length: 20000   epsilon: 0.04561590000080307\n",
      "episode: 271   score: -85.0   memory length: 20000   epsilon: 0.04392440000080279\n",
      "episode: 272   score: -149.0   memory length: 20000   epsilon: 0.04095930000080231\n",
      "episode: 273   score: -174.0   memory length: 20000   epsilon: 0.03749670000080174\n",
      "episode: 274   score: -87.0   memory length: 20000   epsilon: 0.03576540000080146\n",
      "episode: 275   score: -111.0   memory length: 20000   epsilon: 0.0335565000008011\n",
      "episode: 276   score: -92.0   memory length: 20000   epsilon: 0.0317257000008008\n",
      "episode: 277   score: -174.0   memory length: 20000   epsilon: 0.028263100000800758\n",
      "episode: 278   score: -169.0   memory length: 20000   epsilon: 0.024900000000800795\n",
      "episode: 279   score: -95.0   memory length: 20000   epsilon: 0.023009500000800816\n",
      "episode: 280   score: -150.0   memory length: 20000   epsilon: 0.02002450000080085\n",
      "episode: 281   score: -166.0   memory length: 20000   epsilon: 0.016721100000800886\n",
      "episode: 282   score: -87.0   memory length: 20000   epsilon: 0.014989800000800906\n",
      "episode: 283   score: -149.0   memory length: 20000   epsilon: 0.012024700000800938\n",
      "episode: 284   score: -145.0   memory length: 20000   epsilon: 0.00913920000080097\n",
      "episode: 285   score: -145.0   memory length: 20000   epsilon: 0.0062537000008010026\n",
      "episode: 286   score: -154.0   memory length: 20000   epsilon: 0.004980100000801017\n",
      "episode: 287   score: -106.0   memory length: 20000   epsilon: 0.004980100000801017\n",
      "episode: 288   score: -145.0   memory length: 20000   epsilon: 0.004980100000801017\n",
      "episode: 289   score: -88.0   memory length: 20000   epsilon: 0.004980100000801017\n",
      "episode: 290   score: -89.0   memory length: 20000   epsilon: 0.004980100000801017\n",
      "episode: 291   score: -200.0   memory length: 20000   epsilon: 0.004980100000801017\n",
      "episode: 292   score: -86.0   memory length: 20000   epsilon: 0.004980100000801017\n",
      "episode: 293   score: -146.0   memory length: 20000   epsilon: 0.004980100000801017\n",
      "episode: 294   score: -185.0   memory length: 20000   epsilon: 0.004980100000801017\n",
      "episode: 295   score: -157.0   memory length: 20000   epsilon: 0.004980100000801017\n",
      "episode: 296   score: -149.0   memory length: 20000   epsilon: 0.004980100000801017\n",
      "episode: 297   score: -157.0   memory length: 20000   epsilon: 0.004980100000801017\n",
      "episode: 298   score: -146.0   memory length: 20000   epsilon: 0.004980100000801017\n",
      "episode: 299   score: -158.0   memory length: 20000   epsilon: 0.004980100000801017\n"
     ]
    }
   ],
   "source": [
    "for e in range(N_EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "#     print(state)\n",
    "\n",
    "    # Action 0 (left), 1 (do nothing), 3 (declare fake_action to avoid doing nothing\n",
    "    fake_action = 0\n",
    "\n",
    "    # Counter for the same action 4 times\n",
    "    action_count = 0\n",
    "\n",
    "    while not done:\n",
    "        if agent.render:\n",
    "            env.render()\n",
    "\n",
    "        # Select an action in the current state and proceed to a step\n",
    "        action_count = action_count + 1\n",
    "\n",
    "        if action_count == 4:\n",
    "            action = agent.get_action(state)\n",
    "            action_count = 0\n",
    "\n",
    "            if action == 0:\n",
    "                fake_action = 0\n",
    "            elif action == 1:\n",
    "                fake_action = 2\n",
    "\n",
    "        # Take 1 step with the selected action\n",
    "        next_state, reward, done, info = env.step(fake_action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # Give a penalty of -100 for actions that end an episode\n",
    "        # reward = reward if not done else -100\n",
    "\n",
    "        # Save <s, a, r, s'> to replay memory\n",
    "        agent.replay_memory(state, fake_action, reward, next_state, done)\n",
    "        # Continue to learn every time step\n",
    "        agent.train_replay()\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            env.reset()\n",
    "            # Copy the learning model for each episode to the target model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # For each episode, the time step where cartpole stood is plot\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\", len(agent.memory),\n",
    "                  \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "    # Save model for every 50 episodes\n",
    "    if e % 50 == 0:\n",
    "        agent.save_model(\"./save_model/<your_saved_model_name>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
