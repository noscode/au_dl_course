{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein Family Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "family_classification_metadata = pd.read_table('../seminar_5/data/family_classification_metadata.tab')\n",
    "family_classification_sequences = pd.read_table('../seminar_5/data/family_classification_sequences.tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SwissProtAccessionID</th>\n",
       "      <th>LongID</th>\n",
       "      <th>ProteinName</th>\n",
       "      <th>FamilyID</th>\n",
       "      <th>FamilyDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q6GZX4</td>\n",
       "      <td>001R_FRG3G</td>\n",
       "      <td>Putative transcription factor 001R</td>\n",
       "      <td>Pox_VLTF3</td>\n",
       "      <td>Poxvirus Late Transcription Factor VLTF3 like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q6GZX3</td>\n",
       "      <td>002L_FRG3G</td>\n",
       "      <td>Uncharacterized protein 002L</td>\n",
       "      <td>DUF230</td>\n",
       "      <td>Poxvirus proteins of unknown function</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q6GZX0</td>\n",
       "      <td>005R_FRG3G</td>\n",
       "      <td>Uncharacterized protein 005R</td>\n",
       "      <td>US22</td>\n",
       "      <td>US22 like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q91G88</td>\n",
       "      <td>006L_IIV6</td>\n",
       "      <td>Putative KilA-N domain-containing protein 006L</td>\n",
       "      <td>DUF3627</td>\n",
       "      <td>Protein of unknown function (DUF3627)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q197F3</td>\n",
       "      <td>007R_IIV3</td>\n",
       "      <td>Uncharacterized protein 007R</td>\n",
       "      <td>DUF2738</td>\n",
       "      <td>Protein of unknown function (DUF2738)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SwissProtAccessionID      LongID  \\\n",
       "0               Q6GZX4  001R_FRG3G   \n",
       "1               Q6GZX3  002L_FRG3G   \n",
       "2               Q6GZX0  005R_FRG3G   \n",
       "3               Q91G88   006L_IIV6   \n",
       "4               Q197F3   007R_IIV3   \n",
       "\n",
       "                                      ProteinName   FamilyID  \\\n",
       "0              Putative transcription factor 001R  Pox_VLTF3   \n",
       "1                    Uncharacterized protein 002L     DUF230   \n",
       "2                    Uncharacterized protein 005R       US22   \n",
       "3  Putative KilA-N domain-containing protein 006L    DUF3627   \n",
       "4                    Uncharacterized protein 007R    DUF2738   \n",
       "\n",
       "                               FamilyDescription  \n",
       "0  Poxvirus Late Transcription Factor VLTF3 like  \n",
       "1          Poxvirus proteins of unknown function  \n",
       "2                                      US22 like  \n",
       "3          Protein of unknown function (DUF3627)  \n",
       "4          Protein of unknown function (DUF2738)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "family_classification_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSIIGATRLQNDKSDTYSAGPCYAGGCSAFTPRGTCGKDWDLGEQT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MQNPLPEVMSPEHDKRTTTPMSKEANKFIRELDKKPGDLAVVSDFV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDSLNEVCYEQIKGTFYKGLFGDFPLIVDKKTGCFNATKLCVLGGK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEAKNITIDNTTYNFFKFYNINQPLTNLKYLNSERLCFSNAVMGKI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sequences\n",
       "0  MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQV...\n",
       "1  MSIIGATRLQNDKSDTYSAGPCYAGGCSAFTPRGTCGKDWDLGEQT...\n",
       "2  MQNPLPEVMSPEHDKRTTTPMSKEANKFIRELDKKPGDLAVVSDFV...\n",
       "3  MDSLNEVCYEQIKGTFYKGLFGDFPLIVDKKTGCFNATKLCVLGGK...\n",
       "4  MEAKNITIDNTTYNFFKFYNINQPLTNLKYLNSERLCFSNAVMGKI..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "family_classification_sequences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SwissProtAccessionID</th>\n",
       "      <th>LongID</th>\n",
       "      <th>ProteinName</th>\n",
       "      <th>FamilyID</th>\n",
       "      <th>FamilyDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>324018</td>\n",
       "      <td>324018</td>\n",
       "      <td>324018</td>\n",
       "      <td>324018</td>\n",
       "      <td>324018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>287308</td>\n",
       "      <td>295671</td>\n",
       "      <td>56951</td>\n",
       "      <td>7027</td>\n",
       "      <td>6967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Q1X881</td>\n",
       "      <td>POLG_TBEVH</td>\n",
       "      <td>UvrABC system protein B</td>\n",
       "      <td>MMR_HSR1</td>\n",
       "      <td>50S ribosome-binding GTPase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>1500</td>\n",
       "      <td>3084</td>\n",
       "      <td>3084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SwissProtAccessionID      LongID              ProteinName  FamilyID  \\\n",
       "count                324018      324018                   324018    324018   \n",
       "unique               287308      295671                    56951      7027   \n",
       "top                  Q1X881  POLG_TBEVH  UvrABC system protein B  MMR_HSR1   \n",
       "freq                     16          12                     1500      3084   \n",
       "\n",
       "                  FamilyDescription  \n",
       "count                        324018  \n",
       "unique                         6967  \n",
       "top     50S ribosome-binding GTPase  \n",
       "freq                           3084  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "family_classification_metadata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task:\n",
    "    \n",
    "Use your ProtVec embedding from homework 5 to perform protein family classification using RNN.\n",
    "\n",
    "Article with the original research can be found here http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0141287&type=printable\n",
    "\n",
    "* use 1000 most frequent families for classification\n",
    "* validate your results on the train-test split\n",
    "* reduce the dimensionality of the protein-space using Stochastic Neighbor Embedding and visualize two most frequent classes\n",
    "* compare your RNN results with SVM\n",
    "* visualization and metrics are up to you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_FAMILIES = 1000\n",
    "MAX_PROTEIN_LENGTH = 500\n",
    "def get_families(size=NUMBER_OF_FAMILIES):\n",
    "    counter = {}\n",
    "    for id in family_classification_metadata[\"FamilyID\"]:\n",
    "        if id not in counter:\n",
    "            counter[id] = 0\n",
    "        counter[id] += 1\n",
    "    \n",
    "    sorted_families = sorted(counter, key=counter.__getitem__, reverse=True)\n",
    "    return sorted_families[:size]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "families = get_families()\n",
    "families_map = {fam: i for i, fam in enumerate(families)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_prot2vec_from_article():\n",
    "    res_dict = {}\n",
    "    for line in pd.read_csv('../seminar_6/data/protVec_100d_3grams.csv').as_matrix():\n",
    "        split_line = line[0].split()\n",
    "        res_dict[split_line[0]] = np.array([float(x) for x in split_line[1:]])\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prot2vec = process_prot2vec_from_article()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lazy import lazy\n",
    "\n",
    "class SequenceClassificationModel:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self._create_placeholders()\n",
    "        self.prediction\n",
    "        self.cost\n",
    "        self.error\n",
    "        self.optimize\n",
    "        self._create_summaries()\n",
    "    \n",
    "    def _create_placeholders(self):\n",
    "        with tf.name_scope(\"data\"):\n",
    "            self.data = tf.placeholder(tf.float32, [None, self.params.seq_length, self.params.embed_length])\n",
    "            self.target = tf.placeholder(tf.float32, [None, NUMBER_OF_FAMILIES])\n",
    "  \n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar('loss', self.cost)\n",
    "            tf.summary.scalar('erroe', self.error)\n",
    "            self.summary = tf.summary.merge_all()\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "    @lazy\n",
    "    def length(self):\n",
    "    # First, we obtain the lengths of sequences in the current data batch. We need this since\n",
    "    # the data comes as a single tensor, padded with zero vectors to the longest review length.\n",
    "    # Instead of keeping track of the sequence lengths of every review, we just compute it\n",
    "    # dynamically in TensorFlow.\n",
    "    \n",
    "        with tf.name_scope(\"seq_length\"):\n",
    "            used = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "            length = tf.reduce_sum(used, reduction_indices=1)\n",
    "            length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "    \n",
    "    @lazy\n",
    "    def prediction(self):\n",
    "    # Note that the last relevant output activation of the RNN has a different index for each\n",
    "    # sequence in the training batch. This is because each review has a different length. We\n",
    "    # already know the length of each sequence.\n",
    "    # The problem is that we want to index in the dimension of time steps, which is\n",
    "    # the second dimension in the batch of shape  (sequences, time_steps, word_vectors) .\n",
    "    \n",
    "        with tf.name_scope(\"recurrent_layer\"):\n",
    "            output, _ = tf.nn.dynamic_rnn(\n",
    "                self.params.rnn_cell(self.params.rnn_hidden),\n",
    "                self.data,\n",
    "                dtype=tf.float32,\n",
    "                sequence_length=self.length\n",
    "            )\n",
    "        last = self._last_relevant(output, self.length)\n",
    "\n",
    "        with tf.name_scope(\"softmax_layer\"):\n",
    "            num_classes = int(self.target.get_shape()[1])\n",
    "            weight = tf.Variable(tf.truncated_normal(\n",
    "                [self.params.rnn_hidden, num_classes], stddev=0.01))\n",
    "            bias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "            prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)\n",
    "        return prediction\n",
    "    \n",
    "    @lazy\n",
    "    def cost(self):\n",
    "        cross_entropy = -tf.reduce_sum(self.target * tf.log(self.prediction))\n",
    "        return cross_entropy\n",
    "    \n",
    "    @lazy\n",
    "    def error(self):\n",
    "        self.mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 1), tf.argmax(self.prediction, 1))\n",
    "        return tf.reduce_mean(tf.cast(self.mistakes, tf.float32))\n",
    "    \n",
    "    @lazy\n",
    "    def optimize(self):\n",
    "    # RNNs are quite hard to train and weights tend to diverge if the hyper parameters do not\n",
    "    # play nicely together. The idea of gradient clipping is to restrict the the values of the\n",
    "    # gradient to a sensible range. This way, we can limit the maximum weight updates.\n",
    "\n",
    "        with tf.name_scope(\"optimization\"):\n",
    "            gradient = self.params.optimizer.compute_gradients(self.cost)\n",
    "            if self.params.gradient_clipping:\n",
    "                limit = self.params.gradient_clipping\n",
    "                gradient = [\n",
    "                    (tf.clip_by_value(g, -limit, limit), v)\n",
    "                    if g is not None else (None, v)\n",
    "                    for g, v in gradient]\n",
    "            optimize = self.params.optimizer.apply_gradients(gradient)\n",
    "        return optimize\n",
    "    \n",
    "    @staticmethod\n",
    "    def _last_relevant(output, length):\n",
    "        with tf.name_scope(\"last_relevant\"):\n",
    "            # As of now, TensorFlow only supports indexing along the first dimension, using\n",
    "            # tf.gather() . We thus flatten the first two dimensions of the output activations from their\n",
    "            # shape of  sequences x time_steps x word_vectors  and construct an index into this resulting tensor.\n",
    "            batch_size = tf.shape(output)[0]\n",
    "            max_length = int(output.get_shape()[1])\n",
    "            output_size = int(output.get_shape()[2])\n",
    "\n",
    "            # The index takes into account the start indices for each sequence in the flat tensor and adds\n",
    "            # the sequence length to it. Actually, we only add  length - 1  so that we select the last valid\n",
    "            # time step.\n",
    "            index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "            flat = tf.reshape(output, [-1, output_size])\n",
    "            relevant = tf.gather(flat, index)\n",
    "        return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(protein):\n",
    "    embedded = []\n",
    "    for i in range(0, len(protein) - 3, 3):\n",
    "        try:\n",
    "            embedded.append(prot2vec[protein[i:i+3]])\n",
    "        except KeyError:\n",
    "            embedded.append(prot2vec['<unk>'])\n",
    "    return np.array(embedded)\n",
    "    \n",
    "class Embedding:\n",
    "    def __init__(self, length, parser=parse):\n",
    "        self.parser = parser\n",
    "        self._length = length\n",
    "        self.dimensions = 100\n",
    "        \n",
    "    def __call__(self, protein):\n",
    "        embedded = self.parser(protein)\n",
    "        if len(embedded) < self._length:\n",
    "            embedded = np.vstack((embedded, np.zeros(shape=(self._length - embedded.shape[0], self.dimensions))))\n",
    "        return np.array(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proteins = np.array(family_classification_sequences['Sequences'])\n",
    "families = np.array(family_classification_metadata['FamilyID'])\n",
    "\n",
    "length = MAX_PROTEIN_LENGTH\n",
    "\n",
    "embedding = Embedding(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from attrdict import AttrDict\n",
    "\n",
    "params = AttrDict(\n",
    "    rnn_cell=tf.contrib.rnn.GRUCell,\n",
    "    rnn_hidden=256,\n",
    "    optimizer=tf.train.AdamOptimizer(0.001),\n",
    "    batch_size=256,\n",
    "    gradient_clipping=100,\n",
    "    seq_length=length,\n",
    "    embed_length=embedding.dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_target(family):\n",
    "    return np.eye(NUMBER_OF_FAMILIES)[families_map[family]]\n",
    "    \n",
    "def preprocess_batched(iterator, length, embedding, batch_size):\n",
    "    while True:\n",
    "        data = []\n",
    "        target = []\n",
    "        for index in range(batch_size):\n",
    "            try:\n",
    "                protein, family = next(iterator)\n",
    "            except:\n",
    "                return\n",
    "            data.append(embedding(protein))\n",
    "            target.append(build_target(family))\n",
    "        yield np.array(data), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177226\n",
      "19692\n"
     ]
    }
   ],
   "source": [
    "valid_indices = []\n",
    "for i,f in enumerate(families):\n",
    "    if f not in families_map or len(proteins[i]) > MAX_PROTEIN_LENGTH:\n",
    "        continue\n",
    "    valid_indices.append(i)\n",
    "valid_indices = np.array(valid_indices)\n",
    "np.random.shuffle(valid_indices)\n",
    "\n",
    "train_ratio = 0.9\n",
    "\n",
    "train_indices = valid_indices[:int(valid_indices.shape[0] * train_ratio)]\n",
    "test_indices = valid_indices[int(valid_indices.shape[0] * train_ratio):]\n",
    "\n",
    "print(len(train_indices))\n",
    "print(len(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iterator():\n",
    "    for index in train_indices:\n",
    "        yield (proteins[index], families[index])\n",
    "    \n",
    "def test_iterator():\n",
    "    for index in test_indices:\n",
    "        yield (proteins[index], families[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = SequenceClassificationModel(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 99.6%\n",
      "2: 100.0%\n",
      "3: 99.6%\n",
      "4: 100.0%\n",
      "5: 99.6%\n",
      "6: 99.2%\n",
      "7: 98.8%\n",
      "8: 98.0%\n",
      "9: 98.0%\n",
      "10: 98.8%\n",
      "11: 97.7%\n",
      "12: 97.7%\n",
      "13: 98.8%\n",
      "14: 98.8%\n",
      "15: 99.2%\n",
      "16: 98.0%\n",
      "17: 98.4%\n",
      "18: 97.3%\n",
      "19: 97.7%\n",
      "20: 98.4%\n",
      "21: 99.2%\n",
      "22: 98.4%\n",
      "23: 98.8%\n",
      "24: 98.8%\n",
      "25: 98.4%\n",
      "26: 98.4%\n",
      "27: 98.8%\n",
      "28: 98.8%\n",
      "29: 98.8%\n",
      "30: 98.8%\n",
      "31: 99.6%\n",
      "32: 100.0%\n",
      "33: 99.6%\n",
      "34: 98.8%\n",
      "35: 99.6%\n",
      "36: 98.8%\n",
      "37: 99.2%\n",
      "38: 99.2%\n",
      "39: 99.6%\n",
      "40: 98.8%\n",
      "41: 98.8%\n",
      "42: 98.4%\n",
      "43: 97.7%\n",
      "44: 98.4%\n",
      "45: 99.2%\n",
      "46: 98.8%\n",
      "47: 99.2%\n",
      "48: 99.2%\n",
      "49: 100.0%\n",
      "50: 98.8%\n",
      "51: 98.4%\n",
      "52: 97.3%\n",
      "53: 99.2%\n",
      "54: 98.4%\n",
      "55: 98.4%\n",
      "56: 98.4%\n",
      "57: 99.2%\n",
      "58: 98.4%\n",
      "59: 97.7%\n",
      "60: 97.7%\n",
      "61: 99.2%\n",
      "62: 98.4%\n",
      "63: 98.4%\n",
      "64: 98.4%\n",
      "65: 98.8%\n",
      "66: 98.4%\n",
      "67: 97.7%\n",
      "68: 98.0%\n",
      "69: 98.4%\n",
      "70: 96.9%\n",
      "71: 99.6%\n",
      "72: 98.8%\n",
      "73: 99.2%\n",
      "74: 98.4%\n",
      "75: 98.0%\n",
      "76: 99.6%\n",
      "77: 97.7%\n",
      "78: 98.0%\n",
      "79: 97.7%\n",
      "80: 98.0%\n",
      "81: 96.9%\n",
      "82: 96.9%\n",
      "83: 98.0%\n",
      "84: 98.8%\n",
      "85: 98.0%\n",
      "86: 97.7%\n",
      "87: 97.7%\n",
      "88: 97.3%\n",
      "89: 97.7%\n",
      "90: 97.7%\n",
      "91: 97.7%\n",
      "92: 97.7%\n",
      "93: 97.7%\n",
      "94: 98.0%\n",
      "95: 98.8%\n",
      "96: 98.0%\n",
      "97: 96.9%\n",
      "98: 98.0%\n",
      "99: 96.5%\n",
      "100: 98.4%\n",
      "101: 98.8%\n",
      "102: 98.8%\n",
      "103: 98.4%\n",
      "104: 97.7%\n",
      "105: 97.3%\n",
      "106: 97.7%\n",
      "107: 97.3%\n",
      "108: 96.9%\n",
      "109: 97.3%\n",
      "110: 98.4%\n",
      "111: 99.6%\n",
      "112: 99.2%\n",
      "113: 98.4%\n",
      "114: 98.4%\n",
      "115: 96.1%\n",
      "116: 96.9%\n",
      "117: 97.7%\n",
      "118: 96.9%\n",
      "119: 97.3%\n",
      "120: 97.3%\n",
      "121: 96.9%\n",
      "122: 98.0%\n",
      "123: 98.4%\n",
      "124: 97.3%\n",
      "125: 98.8%\n",
      "126: 98.0%\n",
      "127: 97.3%\n",
      "128: 97.7%\n",
      "129: 97.3%\n",
      "130: 97.3%\n",
      "131: 97.3%\n",
      "132: 95.3%\n",
      "133: 98.0%\n",
      "134: 97.3%\n",
      "135: 96.9%\n",
      "136: 95.7%\n",
      "137: 99.2%\n",
      "138: 96.5%\n",
      "139: 96.1%\n",
      "140: 98.4%\n",
      "141: 96.1%\n",
      "142: 95.7%\n",
      "143: 97.3%\n",
      "144: 97.3%\n",
      "145: 96.5%\n",
      "146: 96.5%\n",
      "147: 95.3%\n",
      "148: 97.3%\n",
      "149: 97.3%\n",
      "150: 96.5%\n",
      "151: 97.3%\n",
      "152: 96.1%\n",
      "153: 97.7%\n",
      "154: 96.5%\n",
      "155: 94.5%\n",
      "156: 96.9%\n",
      "157: 94.5%\n",
      "158: 95.3%\n",
      "159: 98.0%\n",
      "160: 94.9%\n",
      "161: 93.8%\n",
      "162: 94.1%\n",
      "163: 97.7%\n",
      "164: 96.9%\n",
      "165: 98.0%\n",
      "166: 96.9%\n",
      "167: 95.3%\n",
      "168: 96.9%\n",
      "169: 96.5%\n",
      "170: 93.4%\n",
      "171: 95.7%\n",
      "172: 94.1%\n",
      "173: 97.7%\n",
      "174: 93.4%\n",
      "175: 98.0%\n",
      "176: 96.9%\n",
      "177: 94.9%\n",
      "178: 96.9%\n",
      "179: 94.9%\n",
      "180: 96.5%\n",
      "181: 95.7%\n",
      "182: 97.7%\n",
      "183: 96.1%\n",
      "184: 96.1%\n",
      "185: 95.7%\n",
      "186: 94.5%\n",
      "187: 93.8%\n",
      "188: 98.0%\n",
      "189: 94.9%\n",
      "190: 96.1%\n",
      "191: 97.3%\n",
      "192: 94.1%\n",
      "193: 94.1%\n",
      "194: 93.0%\n",
      "195: 96.1%\n",
      "196: 95.7%\n",
      "197: 95.7%\n",
      "198: 97.3%\n",
      "199: 95.7%\n",
      "200: 95.7%\n",
      "201: 94.5%\n",
      "202: 96.5%\n",
      "203: 95.3%\n",
      "204: 94.5%\n",
      "205: 97.7%\n",
      "206: 94.9%\n",
      "207: 96.1%\n",
      "208: 95.3%\n",
      "209: 94.5%\n",
      "210: 94.5%\n",
      "211: 94.1%\n",
      "212: 95.3%\n",
      "213: 93.0%\n",
      "214: 97.3%\n",
      "215: 91.0%\n",
      "216: 93.4%\n",
      "217: 89.8%\n",
      "218: 94.1%\n",
      "219: 94.1%\n",
      "220: 95.3%\n",
      "221: 94.9%\n",
      "222: 95.7%\n",
      "223: 94.9%\n",
      "224: 93.4%\n",
      "225: 95.7%\n",
      "226: 96.9%\n",
      "227: 94.9%\n",
      "228: 96.1%\n",
      "229: 93.4%\n",
      "230: 96.5%\n",
      "231: 95.3%\n",
      "232: 95.3%\n",
      "233: 94.1%\n",
      "234: 92.6%\n",
      "235: 93.8%\n",
      "236: 93.8%\n",
      "237: 94.1%\n",
      "238: 95.3%\n",
      "239: 94.1%\n",
      "240: 93.8%\n",
      "241: 93.4%\n",
      "242: 93.8%\n",
      "243: 93.8%\n",
      "244: 94.5%\n",
      "245: 93.4%\n",
      "246: 94.5%\n",
      "247: 93.8%\n",
      "248: 90.6%\n",
      "249: 94.1%\n",
      "250: 92.2%\n",
      "251: 93.8%\n",
      "252: 93.0%\n",
      "253: 92.6%\n",
      "254: 93.8%\n",
      "255: 93.0%\n",
      "256: 92.2%\n",
      "257: 94.5%\n",
      "258: 94.5%\n",
      "259: 93.0%\n",
      "260: 90.2%\n",
      "261: 91.0%\n",
      "262: 91.8%\n",
      "263: 89.8%\n",
      "264: 88.3%\n",
      "265: 91.0%\n",
      "266: 90.6%\n",
      "267: 92.2%\n",
      "268: 91.8%\n",
      "269: 90.2%\n",
      "270: 91.0%\n",
      "271: 91.0%\n",
      "272: 92.2%\n",
      "273: 89.8%\n",
      "274: 93.4%\n",
      "275: 93.8%\n",
      "276: 90.2%\n",
      "277: 84.0%\n",
      "278: 90.2%\n",
      "279: 89.5%\n",
      "280: 88.3%\n",
      "281: 94.9%\n",
      "282: 85.9%\n",
      "283: 89.8%\n",
      "284: 90.2%\n",
      "285: 89.1%\n",
      "286: 91.0%\n",
      "287: 89.1%\n",
      "288: 91.4%\n",
      "289: 90.2%\n",
      "290: 87.9%\n",
      "291: 92.6%\n",
      "292: 89.8%\n",
      "293: 88.7%\n",
      "294: 89.5%\n",
      "295: 89.5%\n",
      "296: 94.5%\n",
      "297: 85.5%\n",
      "298: 88.3%\n",
      "299: 90.6%\n",
      "300: 90.6%\n",
      "301: 92.2%\n",
      "302: 88.7%\n",
      "303: 89.1%\n",
      "304: 92.6%\n",
      "305: 87.9%\n",
      "306: 89.5%\n",
      "307: 90.6%\n",
      "308: 88.7%\n",
      "309: 91.0%\n",
      "310: 85.9%\n",
      "311: 90.2%\n",
      "312: 90.6%\n",
      "313: 90.2%\n",
      "314: 88.7%\n",
      "315: 90.6%\n",
      "316: 90.6%\n",
      "317: 85.9%\n"
     ]
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.25)\n",
    "\n",
    "batches = preprocess_batched(train_iterator(), length, embedding, params.batch_size)\n",
    "iterations = 10000\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter('graphs', sess.graph)\n",
    "\n",
    "    for index, batch in enumerate(batches):\n",
    "        feed = {model.data: batch[0], model.target: batch[1]}\n",
    "        error, _, summary_str = sess.run([model.error, model.optimize, model.summary], feed)\n",
    "        print('{}: {:3.1f}%'.format(index + 1, 100 * error))\n",
    "        if index % 1 == 0:\n",
    "            summary_writer.add_summary(summary_str, index)\n",
    "        if index == iterations:\n",
    "            break\n",
    "    \n",
    "    sum_error = 0.\n",
    "    counter = 0.\n",
    "    \n",
    "    testing = preprocess_batched(test_iterator(), length, embedding, params.batch_size)\n",
    "    for index, batch in enumerate(testing):\n",
    "        feed = {model.data: batch[0], model.target: batch[1]}\n",
    "        error = sess.run(model.error, feed)\n",
    "        sum_error += error\n",
    "        counter += 1\n",
    "        print('Accuracy on testing: {:3.1f}%'.format(100 * (1-sum_error/counter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy on testing: {:3.1f}%'.format(100 * (1-sum_error/counter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sk_parse(protein):\n",
    "    embedded = []\n",
    "    for i in range(0, len(protein) - 3, 3):\n",
    "        try:\n",
    "            embedded.append(prot2vec[protein[i:i+3]])\n",
    "        except KeyError:\n",
    "            embedded.append(prot2vec['<unk>'])\n",
    "    res_embedded = embedded[0]\n",
    "    for i in range(1, len(embedded)):\n",
    "        res_embedded += embedded[i]\n",
    "    res_embedded = np.array(res_embedded)\n",
    "    res_embedded /= len(res_embedded)\n",
    "    return np.array(res_embedded)\n",
    "\n",
    "sk_embedding = Embedding(length, parser=sk_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_proteins_sk(iterator, length, sk_embedding, batch_size):\n",
    "    while True:\n",
    "        data = []\n",
    "        for index in range(batch_size):\n",
    "            try:\n",
    "                protein, family = next(iterator)\n",
    "            except:\n",
    "                return\n",
    "            data.append(sk_embedding(protein))\n",
    "        yield np.array(data)\n",
    "def get_families_sk(iterator, length, sk_embedding, batch_size):\n",
    "    while True:\n",
    "        target = []\n",
    "        for index in range(batch_size):\n",
    "            try:\n",
    "                protein, family = next(iterator)\n",
    "            except:\n",
    "                return\n",
    "            target.append(families_map[family])\n",
    "        yield np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in add\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "sk_train_prot = list(get_proteins_sk(train_iterator(), length, sk_embedding, params.batch_size))\n",
    "sk_train_fams = list(get_families_sk(train_iterator(), length, sk_embedding, params.batch_size))\n",
    "sk_test_prot = get_proteins_sk(test_iterator(), length, sk_embedding, params.batch_size)\n",
    "sk_test_fams = get_families_sk(test_iterator(), length, sk_embedding, params.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=16, max_depth=16)\n",
    "clf.fit(sk_train_prot, sk_train_fams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
